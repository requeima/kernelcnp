{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a23862-5181-4a28-8a1f-7a3e7f7798bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../experiments/synthetic/toy-data/eq-100/data/seed-0/dim-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68855ba8-431c-42e7-84c1-3613d4a98883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5200fc7c-c0c8-4ff6-8b17-70e2d609553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('../experiments/synthetic/toy-data/eq-100/data/seed-0/dim-2/train-data.pkl', 'rb')\n",
    "\n",
    "train_data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ab258e-3995-436e-8d8c-cfe38b352204",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "\n",
    "        epoch = train_data[i]\n",
    "        batch = epoch[j]\n",
    "\n",
    "        x_context = batch[\"x_context\"]\n",
    "        y_context = batch[\"y_context\"]\n",
    "        x_target = batch[\"x_target\"]\n",
    "        y_target = batch[\"y_target\"]\n",
    "        \n",
    "        print(y_context.min(), y_context.max(), y_target.min(), y_target.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4377c0-3852-40f2-810a-3c3bdf706974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import time\n",
    "import sys\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from cnp.experiment import (\n",
    "    generate_root,\n",
    "    WorkingDirectory,\n",
    "    save_checkpoint,\n",
    "    log_args\n",
    ")\n",
    "\n",
    "from cnp.cnp import (\n",
    "    StandardGNP,\n",
    "    StandardAGNP,\n",
    "    StandardConvGNP,\n",
    "    FullConvGNP\n",
    ")\n",
    "\n",
    "from cnp.lnp import (\n",
    "    StandardANP,\n",
    "    StandardConvNP,\n",
    "    StandardHalfUNetConvNP\n",
    ")\n",
    "\n",
    "from cnp.cov import (\n",
    "    InnerProdCov,\n",
    "    KvvCov,\n",
    "    MeanFieldCov,\n",
    "    AddHomoNoise,\n",
    "    AddHeteroNoise,\n",
    "    AddNoNoise\n",
    ")\n",
    "\n",
    "from cnp.oracle import (\n",
    "    eq_cov,\n",
    "    mat_cov,\n",
    "    nm_cov,\n",
    "    wp_cov,\n",
    "    oracle_loglik\n",
    ")\n",
    "\n",
    "from cnp.utils import (\n",
    "    plot_samples_and_data,\n",
    "    make_generator,\n",
    "    Logger\n",
    ")\n",
    "\n",
    "import torch\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Training epoch helper\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def train(data,\n",
    "          model,\n",
    "          optimiser,\n",
    "          log_every,\n",
    "          device,\n",
    "          writer,\n",
    "          iteration):\n",
    "    \n",
    "    for step, batch in enumerate(data):\n",
    "\n",
    "        nll = model.loss(batch['x_context'].to(device),\n",
    "                         batch['y_context'].to(device),\n",
    "                         batch['x_target'].to(device),\n",
    "                         batch['y_target'].to(device))\n",
    "\n",
    "        if step % log_every == 0:\n",
    "            print(f\"Training   neg. log-lik: {nll:.2f}\")\n",
    "\n",
    "        # Compute gradients and apply them\n",
    "        nll.backward()\n",
    "        optimiser.step()\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # Write to tensorboard\n",
    "        writer.add_scalar('Train log-lik.', - nll, iteration)\n",
    "        \n",
    "        iteration = iteration + 1\n",
    "        \n",
    "    return iteration\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Validation helper\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def validate(data,\n",
    "             oracle_cov,\n",
    "             model,\n",
    "             args,\n",
    "             device,\n",
    "             writer,\n",
    "             latent_model):\n",
    "    \n",
    "    # Lists for logging model's training NLL and oracle NLL\n",
    "    nll_list = []\n",
    "    oracle_nll_list = []\n",
    "    \n",
    "    # If training a latent model, set the number of latent samples accordingly\n",
    "    loss_kwargs = {'num_samples' : args.np_val_samples} if latent_model else {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for step, batch in enumerate(data):\n",
    "            \n",
    "            nll = model.loss(batch['x_context'].to(device),\n",
    "                             batch['y_context'].to(device),\n",
    "                             batch['x_target'].to(device),\n",
    "                             batch['y_target'].to(device),\n",
    "                             **loss_kwargs)\n",
    "            \n",
    "            oracle_nll = torch.tensor(0.)\n",
    "\n",
    "            # Oracle loss exists only for GP-generated data, not sawtooth\n",
    "            if oracle_cov is not None:\n",
    "                for b in range(batch['x_context'].shape[0]):\n",
    "                    oracle_nll = oracle_nll - oracle_loglik(batch['x_context'][b],\n",
    "                                                            batch['y_context'][b],\n",
    "                                                            batch['x_target'][b],\n",
    "                                                            batch['y_target'][b],\n",
    "                                                            oracle_cov)[0]\n",
    "                        \n",
    "\n",
    "            # Scale by the average number of target points\n",
    "            nll_list.append(nll.item())\n",
    "            oracle_nll_list.append(oracle_nll.item() / batch['x_context'].shape[0])\n",
    "\n",
    "    mean_nll = np.mean(nll_list)\n",
    "    std_nll = np.var(nll_list)**0.5\n",
    "    \n",
    "    mean_oracle_nll = np.mean(oracle_nll_list)\n",
    "    std_oracle_nll = np.var(oracle_nll_list)**0.5\n",
    "\n",
    "    # Print validation loss and oracle loss\n",
    "    print(f\"Validation neg. log-lik: \"\n",
    "          f\"{mean_nll:.2f}\")\n",
    "\n",
    "    print(f\"Oracle     neg. log-lik: \"\n",
    "          f\"{mean_oracle_nll:.2f}\")\n",
    "\n",
    "    return mean_nll, std_nll, mean_oracle_nll, std_oracle_nll\n",
    "        \n",
    "\n",
    "# Parse arguments given to the script.\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Data generation arguments\n",
    "# =============================================================================\n",
    "\n",
    "parser.add_argument('data', help='Data set to train the CNP on.')\n",
    "\n",
    "parser.add_argument('--x_dim',\n",
    "                    default=1,\n",
    "                    choices=[1, 2],\n",
    "                    type=int,\n",
    "                    help='Input dimension of data.')\n",
    "\n",
    "parser.add_argument('--seed',\n",
    "                    default=0,\n",
    "                    type=int,\n",
    "                    help='Random seed to use.')\n",
    "\n",
    "parser.add_argument('--validate_every',\n",
    "                    default=10,\n",
    "                    type=int,\n",
    "                    help='Number of epochs between validations.')\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Model arguments\n",
    "# =============================================================================\n",
    "\n",
    "parser.add_argument('model',\n",
    "                    choices=['GNP',\n",
    "                             'AGNP',\n",
    "                             'convGNP',\n",
    "                             'FullConvGNP',\n",
    "                             'ANP',\n",
    "                             'convNP',\n",
    "                             'convNPHalfUNet'],\n",
    "                    help='Choice of model. ')\n",
    "\n",
    "parser.add_argument('covtype',\n",
    "                    choices=['innerprod-homo',\n",
    "                             'innerprod-hetero', \n",
    "                             'kvv-homo',\n",
    "                             'kvv-hetero',\n",
    "                             'meanfield'],\n",
    "                    help='Choice of covariance method.')\n",
    "\n",
    "parser.add_argument('--np_loss_samples',\n",
    "                    default=20,\n",
    "                    type=int,\n",
    "                    help='Number of latent samples for evaluating the loss, '\n",
    "                         'used for ANP and ConvNP.')\n",
    "\n",
    "parser.add_argument('--np_val_samples',\n",
    "                    default=8,\n",
    "                    type=int,\n",
    "                    help='Number of latent samples for evaluating the loss, '\n",
    "                         'when validating, used for ANP and ConvNP.')\n",
    "\n",
    "parser.add_argument('--num_basis_dim',\n",
    "                    default=512,\n",
    "                    type=int,\n",
    "                    help='Number of embedding basis dimensions.')\n",
    "\n",
    "parser.add_argument('--learning_rate',\n",
    "                    default=5e-4,\n",
    "                    type=float,\n",
    "                    help='Learning rate.')\n",
    "\n",
    "parser.add_argument('--weight_decay',\n",
    "                    default=0.,\n",
    "                    type=float,\n",
    "                    help='Weight decay.')\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Experiment arguments\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "parser.add_argument('--root',\n",
    "                    help='Experiment root, which is the directory from which '\n",
    "                         'the experiment will run. If it is not given, '\n",
    "                         'a directory will be automatically created.')\n",
    "\n",
    "parser.add_argument('--num_params',\n",
    "                    action='store_true',\n",
    "                    help='Print the total number of parameters in the moodel '\n",
    "                         'and exit.')\n",
    "\n",
    "parser.add_argument('--gpu',\n",
    "                    default=0,\n",
    "                    type=int,\n",
    "                    help='GPU to run experiment on. Defaults to 0.')\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n",
    "    \n",
    "# =============================================================================\n",
    "# Set random seed, device and tensorboard writer\n",
    "# =============================================================================\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "    \n",
    "use_cpu = not torch.cuda.is_available() and args.gpu == 0\n",
    "device = torch.device('cpu') if use_cpu else torch.device('cuda')\n",
    "\n",
    "root = 'experiments/synthetic'\n",
    "\n",
    "# Working directory for saving results\n",
    "experiment_name = os.path.join(f'{root}',\n",
    "                               f'results',\n",
    "                               f'{args.data}',\n",
    "                               f'models',\n",
    "                               f'{args.model}',\n",
    "                               f'{args.covtype}',\n",
    "                               f'seed-{args.seed}',\n",
    "                               f'dim-{args.x_dim}')\n",
    "working_directory = WorkingDirectory(root=experiment_name)\n",
    "\n",
    "# Data directory for loading data\n",
    "data_root = os.path.join(f'{root}',\n",
    "                         f'toy-data',\n",
    "                         f'{args.data}',\n",
    "                         f'data',\n",
    "                         f'seed-{args.seed}',\n",
    "                         f'dim-{args.x_dim}')\n",
    "data_directory = WorkingDirectory(root=data_root)\n",
    "\n",
    "log_path = f'{root}/logs'\n",
    "log_filename = f'{args.data}-{args.model}-{args.covtype}-{args.seed}'\n",
    "log_directory = WorkingDirectory(root=log_path)\n",
    "sys.stdout = Logger(log_directory=log_directory, log_filename=log_filename)\n",
    "\n",
    "# Tensorboard writer\n",
    "writer = SummaryWriter(f'{experiment_name}/log')\n",
    "    \n",
    "file = open(working_directory.file('data_location.txt'), 'w')\n",
    "file.write(data_directory.root)\n",
    "file.close()\n",
    "    \n",
    "\n",
    "# =============================================================================\n",
    "# Create model\n",
    "# =============================================================================\n",
    "\n",
    "# Create covariance method\n",
    "cov = MeanFieldCov(num_basis_dim=1)\n",
    "noise = AddNoNoise()\n",
    "\n",
    "# cov = InnerProdCov(args.num_basis_dim)\n",
    "# noise = AddHomoNoise()\n",
    "    \n",
    "# cov = KvvCov(args.num_basis_dim)\n",
    "# noise = AddHomoNoise()\n",
    "    \n",
    "# Create model architecture\n",
    "# model = StandardGNP(input_dim=args.x_dim,\n",
    "#                     covariance=cov,\n",
    "#                     add_noise=noise)\n",
    "    \n",
    "# model = StandardAGNP(input_dim=args.x_dim,\n",
    "#                      covariance=cov,\n",
    "#                      add_noise=noise)\n",
    "    \n",
    "# model = StandardConvGNP(input_dim=args.x_dim,\n",
    "#                         covariance=cov,\n",
    "#                         add_noise=noise)\n",
    "    \n",
    "\n",
    "elif args.model == 'ANP':\n",
    "    \n",
    "    noise = AddHomoNoise()\n",
    "    model = StandardANP(input_dim=args.x_dim,\n",
    "                        add_noise=noise,\n",
    "                        num_samples=args.np_loss_samples)\n",
    "    \n",
    "elif args.model == 'convNP':\n",
    "    \n",
    "    noise = AddHomoNoise()\n",
    "    model = StandardConvNP(input_dim=args.x_dim,\n",
    "                           add_noise=noise,\n",
    "                           num_samples=args.np_loss_samples)\n",
    "\n",
    "elif args.model == 'convNPHalfUNet':\n",
    "    \n",
    "    noise = AddHomoNoise()\n",
    "    model = StandardHalfUNetConvNP(input_dim=args.x_dim,\n",
    "                                   add_noise=noise,\n",
    "                                   num_samples=args.np_loss_samples)\n",
    "    \n",
    "else:\n",
    "    raise ValueError(f'Unknown model {args.model}.')\n",
    "\n",
    "\n",
    "print(f'{args.model} '\n",
    "      f'{args.covtype} '\n",
    "      f'{args.num_basis_dim}: '\n",
    "      f'{model.num_params}')\n",
    "\n",
    "with open(working_directory.file('num_params.txt'), 'w') as f:\n",
    "    f.write(f'{model.num_params}')\n",
    "        \n",
    "if args.num_params:\n",
    "    exit()\n",
    "    \n",
    "    \n",
    "# Load model to appropriate device\n",
    "model = model.to(device)\n",
    "\n",
    "latent_model = args.model in ['ANP', 'convNP', 'convNPHalfUNet']\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Load data and validation oracle generator\n",
    "# =============================================================================\n",
    "    \n",
    "file = open(data_directory.file('train-data.pkl'), 'rb')\n",
    "data_train = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open(data_directory.file('valid-data.pkl'), 'rb')\n",
    "data_val = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "oracle_cov = None\n",
    "\n",
    "if 'eq' in args.data:\n",
    "    oracle_cov = eq_cov(lengthscale=1.,\n",
    "                        coefficient=1.,\n",
    "                        noise=5e-2)\n",
    "\n",
    "elif 'matern' in args.data:\n",
    "    oracle_cov = mat_cov(lengthscale=1.,\n",
    "                         coefficient=1.,\n",
    "                         noise=5e-2)\n",
    "\n",
    "elif 'noisy-mixture' in args.data:\n",
    "    oracle_cov = nm_cov(lengthscale1=1.,\n",
    "                        lengthscale2=0.25,\n",
    "                        coefficient=1.,\n",
    "                        noise=5e-2)\n",
    "\n",
    "elif 'weakly-periodic' in args.data:\n",
    "    oracle_cov = wp_cov(period=0.25,\n",
    "                        lengthscale=1.,\n",
    "                        coefficient=1.,\n",
    "                        noise=5e-2)\n",
    "\n",
    "elif 'noisy-mixture-slow' in args.data:\n",
    "    oracle_cov = nm_cov(lengthscale1=1.,\n",
    "                        lengthscale2=0.5,\n",
    "                        coefficient=1.,\n",
    "                        noise=5e-2)\n",
    "\n",
    "elif 'weakly-periodic-slow' in args.data:\n",
    "    oracle_cov = wp_cov(period=0.5,\n",
    "                        lengthscale=1.,\n",
    "                        coefficient=1.,\n",
    "                        noise=5e-2)\n",
    "\n",
    "        \n",
    "# =============================================================================\n",
    "# Train or test model\n",
    "# =============================================================================\n",
    "\n",
    "# Number of epochs between validations\n",
    "train_iteration = 0\n",
    "log_every = 500\n",
    "    \n",
    "log_args(working_directory, args)\n",
    "\n",
    "# Create optimiser\n",
    "optimiser = torch.optim.Adam(model.parameters(),\n",
    "                         args.learning_rate,\n",
    "                         weight_decay=args.weight_decay)\n",
    "\n",
    "# Run the training loop, maintaining the best objective value\n",
    "best_nll = np.inf\n",
    "\n",
    "epochs = len(data_train)\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    print('\\nEpoch: {}/{}'.format(epoch + 1, epochs))\n",
    "\n",
    "    if epoch % args.validate_every == 0:\n",
    "\n",
    "        valid_epoch = data_val[epoch // args.validate_every]\n",
    "\n",
    "        # Compute validation negative log-likelihood\n",
    "        val_nll, _, val_oracle, _ = validate(valid_epoch,\n",
    "                                             oracle_cov,\n",
    "                                             model,\n",
    "                                             args,\n",
    "                                             device,\n",
    "                                             writer,\n",
    "                                             latent_model)\n",
    "\n",
    "        # Log information to tensorboard\n",
    "        writer.add_scalar('Valid log-lik.',\n",
    "                          -val_nll,\n",
    "                          epoch)\n",
    "\n",
    "        writer.add_scalar('Valid oracle log-lik.',\n",
    "                          -val_oracle,\n",
    "                          epoch)\n",
    "\n",
    "        writer.add_scalar('Oracle minus valid log-lik.',\n",
    "                          -val_oracle + val_nll,\n",
    "                          epoch)\n",
    "\n",
    "        # Update the best objective value and checkpoint the model\n",
    "        is_best, best_obj = (True, val_nll) if val_nll < best_nll else \\\n",
    "                            (False, best_nll)\n",
    "\n",
    "        plot_marginals = args.covtype == 'meanfield'\n",
    "\n",
    "        if args.x_dim == 1:\n",
    "            \n",
    "            plot_samples_and_data(model=model,\n",
    "                                  valid_epoch=valid_epoch,\n",
    "                                  x_plot_min=-3.,\n",
    "                                  x_plot_max=3.,\n",
    "                                  root=working_directory.root,\n",
    "                                  epoch=epoch,\n",
    "                                  latent_model=latent_model,\n",
    "                                  plot_marginals=plot_marginals,\n",
    "                                  device=device)\n",
    "\n",
    "\n",
    "    train_epoch = data_train[epoch]\n",
    "\n",
    "    # Compute training negative log-likelihood\n",
    "    train_iteration = train(train_epoch,\n",
    "                            model,\n",
    "                            optimiser,\n",
    "                            log_every,\n",
    "                            device,\n",
    "                            writer,\n",
    "                            train_iteration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-gnp",
   "language": "python",
   "name": "venv-gnp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
