{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68855ba8-431c-42e7-84c1-3613d4a98883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stheno import GP, EQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5200fc7c-c0c8-4ff6-8b17-70e2d609553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open('../experiments/synthetic/toy-data/weakly-periodic-100/data/seed-0/dim-2/train-data.pkl', 'rb')\n",
    "\n",
    "# train_data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66ab258e-3995-436e-8d8c-cfe38b352204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(3):\n",
    "#     for j in range(3):\n",
    "\n",
    "#         epoch = train_data[i]\n",
    "#         batch = epoch[j]\n",
    "\n",
    "#         x_context = batch[\"x_context\"]\n",
    "#         y_context = batch[\"y_context\"]\n",
    "#         x_target = batch[\"x_target\"]\n",
    "#         y_target = batch[\"y_target\"]\n",
    "        \n",
    "#         print(\"==============================================================\")\n",
    "#         print(x_context.min(), x_context.max(), x_target.min(), x_target.max())\n",
    "#         print(y_context.min(), y_context.max(), y_target.min(), y_target.max())\n",
    "#         print(x_context.shape, x_context.shape, y_target.shape, y_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3370c49-24df-4855-a9aa-a11b652f8005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch = train_data[0]\n",
    "# batch = epoch[0]\n",
    "\n",
    "# x_context = batch[\"x_context\"]\n",
    "# y_context = batch[\"y_context\"]\n",
    "# x_target = batch[\"x_target\"]\n",
    "# y_target = batch[\"y_target\"]\n",
    "\n",
    "# idx = 10\n",
    "\n",
    "# plt.scatter(x_context[idx, :, 0], x_context[idx, :, 1])\n",
    "# plt.scatter(x_target[idx, :, 0], x_target[idx, :, 1])\n",
    "# plt.show()\n",
    "\n",
    "# plt.hist(y_target[idx, :, 0].numpy().flatten())\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f4377c0-3852-40f2-810a-3c3bdf706974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root: ../experiments/synthetic/results/eq-100/models/GNP/kvv-homo/seed-0/dim-2\n",
      "Root: ../experiments/synthetic/toy-data/eq-100/data/seed-0/dim-2\n",
      "Root: ../experiments/synthetic/logs\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import time\n",
    "import sys\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from cnp.experiment import (\n",
    "    generate_root,\n",
    "    WorkingDirectory,\n",
    "    save_checkpoint,\n",
    "    log_args\n",
    ")\n",
    "\n",
    "from cnp.cnp import (\n",
    "    StandardGNP,\n",
    "    StandardAGNP,\n",
    "    StandardConvGNP,\n",
    "    FullConvGNP\n",
    ")\n",
    "\n",
    "from cnp.lnp import (\n",
    "    StandardANP,\n",
    "    StandardConvNP,\n",
    "    StandardHalfUNetConvNP\n",
    ")\n",
    "\n",
    "from cnp.cov import (\n",
    "    InnerProdCov,\n",
    "    KvvCov,\n",
    "    MeanFieldCov,\n",
    "    AddHomoNoise,\n",
    "    AddHeteroNoise,\n",
    "    AddNoNoise\n",
    ")\n",
    "\n",
    "from cnp.oracle import (\n",
    "    eq_cov,\n",
    "    mat_cov,\n",
    "    nm_cov,\n",
    "    wp_cov,\n",
    "    gp_loglik\n",
    ")\n",
    "\n",
    "from cnp.utils import (\n",
    "    plot_samples_and_data,\n",
    "    make_generator,\n",
    "    Logger\n",
    ")\n",
    "\n",
    "import torch\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Training epoch helper\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def train(data,\n",
    "          model,\n",
    "          optimiser,\n",
    "          log_every,\n",
    "          device,\n",
    "          writer,\n",
    "          iteration):\n",
    "    \n",
    "    for step, batch in enumerate(data):\n",
    "\n",
    "        nll = model.loss(batch['x_context'].to(device),\n",
    "                         batch['y_context'].to(device),\n",
    "                         batch['x_target'].to(device),\n",
    "                         batch['y_target'].to(device))\n",
    "\n",
    "        if step % log_every == 0:\n",
    "            print(f\"Training   neg. log-lik: {nll:.2f}\")\n",
    "\n",
    "        # Compute gradients and apply them\n",
    "        nll.backward()\n",
    "        optimiser.step()\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # Write to tensorboard\n",
    "        writer.add_scalar('Train log-lik.', - nll, iteration)\n",
    "        \n",
    "        iteration = iteration + 1\n",
    "        \n",
    "    return iteration\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Validation helper\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def validate(data,\n",
    "             oracle_cov,\n",
    "             model,\n",
    "             args_np_val_samples,\n",
    "             device,\n",
    "             writer,\n",
    "             latent_model):\n",
    "    \n",
    "    # Lists for logging model's training NLL and oracle NLL\n",
    "    nll_list = []\n",
    "    oracle_nll_list = []\n",
    "    \n",
    "    # If training a latent model, set the number of latent samples accordingly\n",
    "    loss_kwargs = {'num_samples' : args_np_val_samples} if latent_model else {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for step, batch in enumerate(data):\n",
    "            \n",
    "            nll = model.loss(batch['x_context'].to(device),\n",
    "                             batch['y_context'].to(device),\n",
    "                             batch['x_target'].to(device),\n",
    "                             batch['y_target'].to(device),\n",
    "                             **loss_kwargs)\n",
    "            \n",
    "            oracle_nll = torch.tensor(0.)\n",
    "\n",
    "            # Oracle loss exists only for GP-generated data, not sawtooth\n",
    "            if oracle_cov is not None:\n",
    "                for b in range(batch['x_context'].shape[0]):\n",
    "                    oracle_nll = oracle_nll - gp_loglik(batch['x_context'][b],\n",
    "                                                            batch['y_context'][b],\n",
    "                                                            batch['x_target'][b],\n",
    "                                                            batch['y_target'][b],\n",
    "                                                            oracle_cov)[0]\n",
    "                        \n",
    "\n",
    "            # Scale by the average number of target points\n",
    "            nll_list.append(nll.item())\n",
    "            oracle_nll_list.append(oracle_nll.item() / batch['x_context'].shape[0])\n",
    "\n",
    "    mean_nll = np.mean(nll_list)\n",
    "    std_nll = np.var(nll_list)**0.5\n",
    "    \n",
    "    mean_oracle_nll = np.mean(oracle_nll_list)\n",
    "    std_oracle_nll = np.var(oracle_nll_list)**0.5\n",
    "\n",
    "    # Print validation loss and oracle loss\n",
    "    print(f\"Validation neg. log-lik: \"\n",
    "          f\"{mean_nll:.2f}\")\n",
    "\n",
    "    print(f\"Oracle     neg. log-lik: \"\n",
    "          f\"{mean_oracle_nll:.2f}\")\n",
    "\n",
    "    return mean_nll, std_nll, mean_oracle_nll, std_oracle_nll\n",
    "\n",
    "args_data = 'eq-100'\n",
    "args_x_dim = 2\n",
    "args_seed = 0\n",
    "args_validate_every = 10\n",
    "args_model = 'GNP'\n",
    "args_covtype = 'kvv-homo'\n",
    "args_np_loss_samples = 20\n",
    "args_np_val_samples = 8\n",
    "args_num_basis_dim = 512\n",
    "args_learning_rate = 5e-4\n",
    "args_weight_decay = 0.\n",
    "args_num_params = False\n",
    "args_gpu = 1\n",
    "    \n",
    "# =============================================================================\n",
    "# Set random seed, device and tensorboard writer\n",
    "# =============================================================================\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(args_seed)\n",
    "torch.manual_seed(args_seed)\n",
    "\n",
    "# Set device\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(args_gpu)\n",
    "    \n",
    "use_cpu = not torch.cuda.is_available() and args_gpu == 0\n",
    "device = torch.device('cpu') if use_cpu else torch.device('cuda')\n",
    "\n",
    "root = '../experiments/synthetic'\n",
    "\n",
    "# Working directory for saving results\n",
    "experiment_name = os.path.join(f'{root}',\n",
    "                               f'results',\n",
    "                               f'{args_data}',\n",
    "                               f'models',\n",
    "                               f'{args_model}',\n",
    "                               f'{args_covtype}',\n",
    "                               f'seed-{args_seed}',\n",
    "                               f'dim-{args_x_dim}')\n",
    "working_directory = WorkingDirectory(root=experiment_name)\n",
    "\n",
    "# Data directory for loading data\n",
    "data_root = os.path.join(f'{root}',\n",
    "                         f'toy-data',\n",
    "                         f'{args_data}',\n",
    "                         f'data',\n",
    "                         f'seed-{args_seed}',\n",
    "                         f'dim-{args_x_dim}')\n",
    "data_directory = WorkingDirectory(root=data_root)\n",
    "\n",
    "log_path = f'{root}/logs'\n",
    "log_filename = f'{args_data}-{args_model}-{args_covtype}-{args_seed}'\n",
    "log_directory = WorkingDirectory(root=log_path)\n",
    "# sys.stdout = Logger(log_directory=log_directory, log_filename=log_filename)\n",
    "\n",
    "# Tensorboard writer\n",
    "writer = SummaryWriter(f'{experiment_name}/log')\n",
    "    \n",
    "file = open(working_directory.file('data_location.txt'), 'w')\n",
    "file.write(data_directory.root)\n",
    "file.close()\n",
    "\n",
    "# =============================================================================\n",
    "# Load data and validation oracle generator\n",
    "# =============================================================================\n",
    "    \n",
    "file = open(data_directory.file('train-data.pkl'), 'rb')\n",
    "data_train = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open(data_directory.file('valid-data.pkl'), 'rb')\n",
    "data_val = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "oracle_cov = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd87ee26-a9ff-4d2a-abcb-9b1e77bda615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.distributions import MultivariateNormal\n",
    "\n",
    "from cnp.encoders import (\n",
    "    StandardEncoder,\n",
    "    StandardANPEncoder,\n",
    "    StandardConvNPEncoder\n",
    ")\n",
    "\n",
    "from cnp.decoders import (\n",
    "    StandardDecoder,\n",
    "    ConvDecoder\n",
    ")\n",
    "\n",
    "from cnp.architectures import (\n",
    "    UNet,\n",
    "    HalfUNet,\n",
    "    StandardDepthwiseSeparableCNN\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# General Latent Neural Process\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class LatentNeuralProcess(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, encoder, decoder, add_noise, num_samples):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.add_noise = add_noise\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    \n",
    "    def forward(self, x_context, y_context, x_target, num_samples=None):\n",
    "        \n",
    "        num_samples = self.num_samples if num_samples is None else num_samples\n",
    "        \n",
    "        # Pass context set and target inputs through the encoder to obtain\n",
    "        # the encoder output, as expected by encoder.sample\n",
    "        encoder_forward_output = self.encoder(x_context, y_context, x_target)\n",
    "        \n",
    "        means = []\n",
    "        noise_vars = []\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            \n",
    "            r = self.encoder.sample(encoder_forward_output)\n",
    "            output = self.decoder(r, x_context, y_context, x_target)\n",
    "            \n",
    "            assert (len(output.shape) == 3) and (output.shape[2] == 2)\n",
    "            \n",
    "            mean = output[:, :, :1]\n",
    "            noise_var = torch.exp(output[:, :, 1])\n",
    "            noise_var = torch.diag_embed(noise_var)\n",
    "            noise_var = self.add_noise(noise_var, None)\n",
    "            \n",
    "            means.append(mean)\n",
    "            noise_vars.append(noise_var)\n",
    "            \n",
    "        means = torch.stack(means, dim=0)\n",
    "        noise_vars = torch.stack(noise_vars, dim=0)\n",
    "        \n",
    "        return means, noise_vars\n",
    "    \n",
    "    \n",
    "    def loss(self, x_context, y_context, x_target, y_target, num_samples=None):\n",
    "        \n",
    "        B = y_target.shape[0]\n",
    "        \n",
    "        num_samples = self.num_samples if num_samples is None else num_samples\n",
    "        \n",
    "        # Compute mean and variance tensors, each of shape (S, B, N, D)\n",
    "        means, noise_vars = self.forward(x_context,\n",
    "                                         y_context,\n",
    "                                         x_target,\n",
    "                                         num_samples=num_samples)\n",
    "        \n",
    "        means = means[:, :, :, 0]\n",
    "        idx = torch.arange(noise_vars.shape[2])\n",
    "        noise_vars = noise_vars[:, :, idx, idx]\n",
    "        \n",
    "        logprobs = []\n",
    "        \n",
    "        for mean, noise_var in zip(means, noise_vars):\n",
    "            \n",
    "            distribution = torch.distributions.Normal(loc=mean,\n",
    "                                                      scale=noise_var**0.5)\n",
    "            logprob = torch.sum(distribution.log_prob(y_target[:, :, 0]),\n",
    "                                axis=-1)\n",
    "            \n",
    "            logprobs.append(logprob)\n",
    "            \n",
    "        logprobs = torch.stack(logprobs, axis=1)\n",
    "        logprob = 0\n",
    "        \n",
    "        for i, batch_logprobs in enumerate(logprobs):\n",
    "            \n",
    "            max_batch_logprob = torch.max(batch_logprobs)\n",
    "        \n",
    "            batch_logprobs = batch_logprobs - max_batch_logprob\n",
    "            \n",
    "            batch_mix_logprob = torch.log(torch.mean(torch.exp(batch_logprobs)))\n",
    "            batch_mix_logprob = batch_mix_logprob + max_batch_logprob\n",
    "            \n",
    "            logprob = logprob + batch_mix_logprob\n",
    "        \n",
    "        return - logprob / B\n",
    "    \n",
    "\n",
    "    def mean_and_marginals(self, x_context, y_context, x_target):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    @property\n",
    "    def num_params(self):\n",
    "        \"\"\"Number of parameters.\"\"\"\n",
    "    \n",
    "        return np.sum([torch.tensor(param.shape).prod() \\\n",
    "                       for param in self.parameters()])\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Attentive Latent Neural Process\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class StandardANP(LatentNeuralProcess):\n",
    "    \n",
    "    \n",
    "    def __init__(self, input_dim, add_noise, num_samples):\n",
    "        \n",
    "        # Standard input/output dim and latent representation dim\n",
    "        # latent_dim is common to stochastic and deterministic paths, and\n",
    "        # these are concatenated, producing a (2 * latent_dim) representation\n",
    "        output_dim = 2\n",
    "        latent_dim = 128\n",
    "        \n",
    "        # Decoder output dimension\n",
    "        decoder_output_dim = output_dim\n",
    "\n",
    "        # Construct the standard encoder\n",
    "        encoder = StandardANPEncoder(input_dim=input_dim,\n",
    "                                     latent_dim=latent_dim)\n",
    "        \n",
    "        # Construct the standard decoder\n",
    "        decoder = StandardDecoder(input_dim=input_dim,\n",
    "                                  latent_dim=2*latent_dim,\n",
    "                                  output_dim=decoder_output_dim)\n",
    "\n",
    "        super().__init__(encoder=encoder,\n",
    "                         decoder=decoder,\n",
    "                         add_noise=add_noise,\n",
    "                         num_samples=num_samples)\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.latent_dim = latent_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f487e0a-b444-42ab-9613-08ea9b8b6647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gp_loglik(xc, yc, xt, yt, covariance, noise=0.05**2):\n",
    "    \n",
    "    p = GP(covariance)\n",
    "    p_post = p | (p(xc, noise), yc)\n",
    "    \n",
    "    loglik = pred.logpdf(y_target)\n",
    "    \n",
    "    return loglik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "361d71ee-ae1e-40bb-b8d4-1f5772960d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNP kvv-homo 512: 116610\n",
      "\n",
      "Epoch: 1/101\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__call__() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-5a051c0931c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    125\u001b[0m                                              \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                                              \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                                              latent_model)\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m# Log information to tensorboard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-755cddd786ea>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(data, oracle_cov, model, args_np_val_samples, device, writer, latent_model)\u001b[0m\n\u001b[1;32m    132\u001b[0m                                                             \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x_target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                                                             \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                                                             oracle_cov)[0]\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-ae90294e69d4>\u001b[0m in \u001b[0;36mgp_loglik\u001b[0;34m(xc, yc, xt, yt, covariance, noise)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcovariance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mp_post\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mloglik\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogpdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __call__() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Create model\n",
    "# =============================================================================\n",
    "\n",
    "# Create covariance method\n",
    "cov = MeanFieldCov(num_basis_dim=1)\n",
    "noise = AddNoNoise()\n",
    "\n",
    "# cov = InnerProdCov(args_num_basis_dim)\n",
    "# noise = AddHomoNoise()\n",
    "    \n",
    "# cov = KvvCov(args_num_basis_dim)\n",
    "# noise = AddHomoNoise()\n",
    "    \n",
    "# Create model architecture\n",
    "\n",
    "if args_model == 'GNP':\n",
    "    model = StandardGNP(input_dim=args_x_dim,\n",
    "                        covariance=cov,\n",
    "                        add_noise=noise)\n",
    "\n",
    "elif args_model == 'AGNP':\n",
    "    model = StandardAGNP(input_dim=args_x_dim,\n",
    "                         covariance=cov,\n",
    "                         add_noise=noise)\n",
    "\n",
    "elif args_model == 'convGNP':\n",
    "    model = StandardConvGNP(input_dim=args_x_dim,\n",
    "                            covariance=cov,\n",
    "                            add_noise=noise)\n",
    "\n",
    "elif args_model == 'ANP':\n",
    "    \n",
    "    noise = AddHomoNoise()\n",
    "    model = StandardANP(input_dim=args_x_dim,\n",
    "                        add_noise=noise,\n",
    "                        num_samples=args_np_loss_samples)\n",
    "    \n",
    "elif args_model == 'convNP':\n",
    "    \n",
    "    noise = AddHomoNoise()\n",
    "    model = StandardConvNP(input_dim=args_x_dim,\n",
    "                           add_noise=noise,\n",
    "                           num_samples=args_np_loss_samples)\n",
    "    \n",
    "else:\n",
    "    raise ValueError(f'Unknown model {args_model}.')\n",
    "\n",
    "\n",
    "print(f'{args_model} '\n",
    "      f'{args_covtype} '\n",
    "      f'{args_num_basis_dim}: '\n",
    "      f'{model.num_params}')\n",
    "\n",
    "with open(working_directory.file('num_params.txt'), 'w') as f:\n",
    "    f.write(f'{model.num_params}')\n",
    "        \n",
    "if args_num_params:\n",
    "    exit()\n",
    "    \n",
    "    \n",
    "# Load model to appropriate device\n",
    "model = model.to(device)\n",
    "\n",
    "latent_model = args_model in ['ANP', 'convNP', 'convNPHalfUNet']\n",
    "\n",
    "# if 'eq' in args_data:\n",
    "#     oracle_cov = eq_cov(lengthscale=1.,\n",
    "#                         coefficient=1.,\n",
    "#                         noise=5e-2)\n",
    "\n",
    "# elif 'matern' in args_data:\n",
    "#     oracle_cov = mat_cov(lengthscale=1.,\n",
    "#                          coefficient=1.,\n",
    "#                          noise=5e-2)\n",
    "\n",
    "# elif 'noisy-mixture' in args_data:\n",
    "#     oracle_cov = nm_cov(lengthscale1=1.,\n",
    "#                         lengthscale2=0.25,\n",
    "#                         coefficient=1.,\n",
    "#                         noise=5e-2)\n",
    "\n",
    "# elif 'weakly-periodic' in args_data:\n",
    "#     oracle_cov = wp_cov(period=0.25,\n",
    "#                         lengthscale=1.,\n",
    "#                         coefficient=1.,\n",
    "#                         noise=5e-2)\n",
    "\n",
    "if 'eq' in args_data:\n",
    "    oracle_cov = EQ().stretch(1.)\n",
    "\n",
    "        \n",
    "# =============================================================================\n",
    "# Train or test model\n",
    "# =============================================================================\n",
    "\n",
    "# Number of epochs between validations\n",
    "train_iteration = 0\n",
    "log_every = 500\n",
    "\n",
    "# Create optimiser\n",
    "optimiser = torch.optim.Adam(model.parameters(),\n",
    "                         args_learning_rate,\n",
    "                         weight_decay=args_weight_decay)\n",
    "\n",
    "# Run the training loop, maintaining the best objective value\n",
    "best_nll = np.inf\n",
    "\n",
    "epochs = len(data_train)\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    print('\\nEpoch: {}/{}'.format(epoch + 1, epochs))\n",
    "\n",
    "    if epoch % args_validate_every == 0:\n",
    "\n",
    "        valid_epoch = data_val[epoch // args_validate_every]\n",
    "\n",
    "        # Compute validation negative log-likelihood\n",
    "        val_nll, _, val_oracle, _ = validate(valid_epoch,\n",
    "                                             oracle_cov,\n",
    "                                             model,\n",
    "                                             args_np_val_samples,\n",
    "                                             device,\n",
    "                                             writer,\n",
    "                                             latent_model)\n",
    "\n",
    "        # Log information to tensorboard\n",
    "        writer.add_scalar('Valid log-lik.',\n",
    "                          -val_nll,\n",
    "                          epoch)\n",
    "\n",
    "        writer.add_scalar('Valid oracle log-lik.',\n",
    "                          -val_oracle,\n",
    "                          epoch)\n",
    "\n",
    "        writer.add_scalar('Oracle minus valid log-lik.',\n",
    "                          -val_oracle + val_nll,\n",
    "                          epoch)\n",
    "\n",
    "        # Update the best objective value and checkpoint the model\n",
    "        is_best, best_obj = (True, val_nll) if val_nll < best_nll else \\\n",
    "                            (False, best_nll)\n",
    "\n",
    "        plot_marginals = args_covtype == 'meanfield'\n",
    "\n",
    "        if args_x_dim == 1:\n",
    "            \n",
    "            plot_samples_and_data(model=model,\n",
    "                                  valid_epoch=valid_epoch,\n",
    "                                  x_plot_min=-3.,\n",
    "                                  x_plot_max=3.,\n",
    "                                  root=working_directory.root,\n",
    "                                  epoch=epoch,\n",
    "                                  latent_model=latent_model,\n",
    "                                  plot_marginals=plot_marginals,\n",
    "                                  device=device)\n",
    "\n",
    "\n",
    "    train_epoch = data_train[epoch]\n",
    "\n",
    "    # Compute training negative log-likelihood\n",
    "    train_iteration = train(train_epoch,\n",
    "                            model,\n",
    "                            optimiser,\n",
    "                            log_every,\n",
    "                            device,\n",
    "                            writer,\n",
    "                            train_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41f57e0f-8606-4830-bf12-041745d753dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from stheno import *\n",
    "from cnp.oracle import oracle_loglik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fe242bd-fa0f-4f68-a3ff-4ded4cfbdc9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.5705792788326036"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengthscale = 1.\n",
    "noise = 5e-2\n",
    "\n",
    "cov = EQ().stretch(lengthscale)\n",
    "p = GP(cov)\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "x_ctx = np.random.rand(3, 2)\n",
    "y_ctx = np.random.rand(3)\n",
    "x_trg = np.random.rand(10, 2)\n",
    "y_trg = np.random.rand(10)\n",
    "\n",
    "oracle_loglik(x_ctx, y_ctx, x_trg, y_trg, cov, noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e95479-273a-4a65-bd88-9064361ee8c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-kernelcnp",
   "language": "python",
   "name": "venv-kernelcnp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
