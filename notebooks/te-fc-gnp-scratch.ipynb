{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import sample_datasets_from_gps, plot_samples_and_predictions\n",
    "from torch.distributions import MultivariateNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Fully Connected Neural Network\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class FullyConnectedNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 output_dim,\n",
    "                 hidden_dims,\n",
    "                 nonlinearity):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        shapes = [input_dim] + hidden_dims + [output_dim]\n",
    "        shapes = [(s1, s2) for s1, s2 in zip(shapes[:-1], shapes[1:])]\n",
    "        \n",
    "        self.W = []\n",
    "        self.b = []\n",
    "        self.num_linear = len(hidden_dims) + 1\n",
    "        \n",
    "        for shape in shapes:\n",
    "\n",
    "            W = nn.Parameter(torch.randn(size=shape) / shape[0] ** 0.5)\n",
    "            b = nn.Parameter(torch.randn(size=shape[1:]))\n",
    "\n",
    "            self.W.append(W)\n",
    "            self.b.append(b)\n",
    "            \n",
    "        self.W = torch.nn.ParameterList(self.W)\n",
    "        self.b = torch.nn.ParameterList(self.b)\n",
    "        \n",
    "        self.nonlinearity = getattr(nn, nonlinearity)()\n",
    "        \n",
    "    \n",
    "    def forward(self, tensor):\n",
    "        \n",
    "        for i, (W, b) in enumerate(zip(self.W, self.b)):\n",
    "            \n",
    "            tensor = torch.einsum('...i, ij -> ...j', tensor, W)\n",
    "                \n",
    "            tensor = tensor + b[(None,) * (len(tensor.shape) - 1)]\n",
    "            \n",
    "            if i < self.num_linear - 1:\n",
    "                tensor = self.nonlinearity(tensor)\n",
    "        \n",
    "        return tensor\n",
    "\n",
    "\n",
    "    \n",
    "# =============================================================================\n",
    "# Gaussian Neural Process\n",
    "# =============================================================================\n",
    "    \n",
    "    \n",
    "class GaussianNeuralProcess(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 encoder,\n",
    "                 decoder,\n",
    "                 log_noise):\n",
    "        \n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "        self.log_noise = torch.tensor(log_noise)\n",
    "        self.log_noise = nn.Parameter(self.log_noise)\n",
    "        \n",
    "        \n",
    "    def forward(self,\n",
    "                ctx_in,\n",
    "                ctx_out,\n",
    "                trg_in):\n",
    "        \n",
    "        D = ctx_out.shape[-1]\n",
    "        \n",
    "        ctx = torch.cat([ctx_in, ctx_out], dim=-1)\n",
    "        \n",
    "        theta = self.encoder(ctx)\n",
    "        theta = torch.mean(theta, dim=1)[:, None, :]\n",
    "        theta = theta.repeat(1, trg_in.shape[1], 1)\n",
    "        \n",
    "        tensor = torch.cat([trg_in, theta], dim=-1)\n",
    "        tensor = self.decoder(tensor)\n",
    "        \n",
    "        mean = tensor[:, :, :1]\n",
    "        log_noise = tensor[:, :, D:(2*D)]\n",
    "        \n",
    "        cov_root = tensor[:, :, (2*D):]\n",
    "        cov = torch.einsum('bni, bmi -> bnm', cov_root, cov_root) / cov_root.shape[-1]\n",
    "        \n",
    "        cov_plus_noise = cov + torch.exp(self.log_noise) * torch.eye(cov.shape[1])[None, ...]\n",
    "                \n",
    "        return mean, cov, cov_plus_noise\n",
    "    \n",
    "    \n",
    "    def _loss(self,\n",
    "              ctx_in,\n",
    "              ctx_out,\n",
    "              trg_in,\n",
    "              trg_out):\n",
    "        \n",
    "        mean, cov, cov_plus_noise = self.forward(ctx_in, ctx_out, trg_in)\n",
    "        \n",
    "        pred_dist = MultivariateNormal(loc=mean[:, :, 0],\n",
    "                                       covariance_matrix=cov_plus_noise)\n",
    "        \n",
    "        log_prob = pred_dist.log_prob(trg_out[:, :, 0])\n",
    "        log_prob = torch.mean(log_prob)\n",
    "        \n",
    "        return - log_prob\n",
    "    \n",
    "    \n",
    "    def loss(self,\n",
    "             inputs,\n",
    "             outputs,\n",
    "             num_samples):\n",
    "        \n",
    "        loss = 0\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            \n",
    "            N = np.random.choice(np.arange(1, inputs.shape[1]))\n",
    "            \n",
    "            ctx_in = inputs[:, :N]\n",
    "            ctx_out = outputs[:, :N]\n",
    "            trg_in = inputs[:, N:]\n",
    "            trg_out = outputs[:, N:]\n",
    "            \n",
    "            loss = loss + self._loss(ctx_in,\n",
    "                                     ctx_out,\n",
    "                                     trg_in,\n",
    "                                     trg_out)\n",
    "        \n",
    "        loss = loss / (num_samples * inputs.shape[1])\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "    \n",
    "# =============================================================================\n",
    "# Translation Equivariant Gaussian Neural Process\n",
    "# =============================================================================\n",
    "\n",
    "    \n",
    "class TranslationEquivariantGaussianNeuralProcess(GaussianNeuralProcess):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 encoder,\n",
    "                 decoder,\n",
    "                 log_noise):\n",
    "        \n",
    "        \n",
    "        super().__init__(encoder=encoder,\n",
    "                         decoder=decoder,\n",
    "                         log_noise=log_noise)\n",
    "        \n",
    "        \n",
    "    def forward(self,\n",
    "                ctx_in,\n",
    "                ctx_out,\n",
    "                trg_in):\n",
    "        \n",
    "        D = ctx_out.shape[-1]\n",
    "        \n",
    "        # Context and target inputs\n",
    "        ctx_in = ctx_in[:, None, :, :]\n",
    "        trg_in = trg_in[:, :, None, :]\n",
    "        \n",
    "        diff = ctx_in - trg_in\n",
    "        \n",
    "        ctx_out = ctx_out[:, None, :, :]\n",
    "        ctx_out = ctx_out.repeat(1, diff.shape[1], 1, 1)\n",
    "        \n",
    "        ctx = torch.cat([diff, ctx_out], dim=-1)\n",
    "        \n",
    "        tensor = self.encoder(ctx)\n",
    "        tensor = torch.mean(tensor, dim=2) # (B, T, R)\n",
    "        \n",
    "        tensor = self.decoder(tensor)\n",
    "        \n",
    "        mean = tensor[:, :, :1]\n",
    "        cov_root = tensor[:, :, 1:]\n",
    "        cov = torch.einsum('bni, bmi -> bnm', cov_root, cov_root) / cov_root.shape[-1]\n",
    "        \n",
    "        diag_noise = torch.exp(self.log_noise) * torch.eye(cov.shape[1])[None, :, :]\n",
    "        cov_plus_noise = cov + diag_noise\n",
    "                \n",
    "        return mean, cov, cov_plus_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Covariance(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_basis_dim, extra_cov_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_basis_dim = num_basis_dim\n",
    "        self.extra_cov_dim = extra_cov_dim\n",
    "\n",
    "\n",
    "class InnerProdCov(Covariance):\n",
    "    \n",
    "    def __init__(self, num_basis_dim):\n",
    "        # Extra dimension to add to the output\n",
    "        extra_cov_dim = 0\n",
    "        super().__init__(num_basis_dim, extra_cov_dim)\n",
    "        \n",
    "    def forward(self, embeddings):\n",
    "        # Compute the covariance by taking inner products between embeddings\n",
    "        basis_emb = embeddings[:, :, :self.num_basis_dim]\n",
    "        cov = torch.einsum('bni, bmi -> bnm', basis_emb, basis_emb) / self.num_basis_dim\n",
    "        \n",
    "        return cov\n",
    "    \n",
    "    \n",
    "\n",
    "class AddNoise(nn.Module):\n",
    "    def __init__(self, extra_noise_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.extra_noise_dim = extra_noise_dim\n",
    "        \n",
    "        \n",
    "class AddHomoNoise(AddNoise):\n",
    "    def __init__(self):\n",
    "        # Extra dimension to add to the output\n",
    "        extra_noise_dim = 0\n",
    "        super().__init__(extra_noise_dim)\n",
    "\n",
    "        # Noise Parameters\n",
    "        self.noise_scale = nn.Parameter(torch.zeros(1), requires_grad=True)\n",
    "    \n",
    "    def forward(self, cov, embeddings):\n",
    "        noise_var = torch.eye(cov.shape[1])[None, ...]\n",
    "        cov_plus_noise = cov + torch.exp(self.noise_scale) * noise_var\n",
    "        \n",
    "        return cov_plus_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNeuralProcess(nn.Module):\n",
    "    \"\"\"Conditional Neural Process Module.\n",
    "\n",
    "    Args:\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, covariance, add_noise):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.covariance = covariance\n",
    "        self.add_noise = add_noise\n",
    "\n",
    "    \n",
    "    def forward(self, x_context, y_context, x_target):\n",
    "        \n",
    "        r = self.encoder(x_context, y_context, x_target)\n",
    "        z = self.decoder(r, x_context, y_context, x_target)\n",
    "        \n",
    "        # Produce mean\n",
    "        mean = z[..., 0:1]\n",
    "        \n",
    "        # Produce cov\n",
    "        embedding = z[..., 1:]\n",
    "        cov = self.covariance(embedding)\n",
    "        cov_plus_noise = self.add_noise(cov, embedding)\n",
    "        \n",
    "        return mean, cov, cov_plus_noise \n",
    "    \n",
    "    \n",
    "class StandardFullyConnectedTEGNP(GaussianNeuralProcess):\n",
    "    \n",
    "    def __init__(self, covariance, add_noise):\n",
    "        \n",
    "        input_dim = 1\n",
    "        output_dim = 1\n",
    "        rep_dim = 128\n",
    "        embedding_dim = output_dim +               \\\n",
    "                        covariance.num_basis_dim + \\\n",
    "                        covariance.extra_cov_dim + \\\n",
    "                        add_noise.extra_noise_dim\n",
    "        \n",
    "        encoder = StandardFullyConnectedTEEncoder(input_dim=input_dim,\n",
    "                                                  output_dim=output_dim,\n",
    "                                                  rep_dim=rep_dim)\n",
    "        \n",
    "        decoder = StandardFullyConnectedTEDecoder(input_dim=input_dim,\n",
    "                                                  output_dim=output_dim,\n",
    "                                                  rep_dim=rep_dim,\n",
    "                                                  embedding_dim=embedding_dim)\n",
    "        \n",
    "        super().__init__(encoder=encoder, \n",
    "                         decoder=decoder,\n",
    "                         covariance=covariance,\n",
    "                         add_noise=add_noise)\n",
    "    \n",
    "    \n",
    "    def _loss(self,\n",
    "              ctx_in,\n",
    "              ctx_out,\n",
    "              trg_in,\n",
    "              trg_out):\n",
    "        \n",
    "        mean, cov, cov_plus_noise = self.forward(ctx_in, ctx_out, trg_in)\n",
    "        \n",
    "        pred_dist = MultivariateNormal(loc=mean[:, :, 0],\n",
    "                                       covariance_matrix=cov_plus_noise)\n",
    "        \n",
    "        log_prob = pred_dist.log_prob(trg_out[:, :, 0])\n",
    "        log_prob = torch.mean(log_prob)\n",
    "        \n",
    "        return - log_prob\n",
    "    \n",
    "    \n",
    "    def loss(self,\n",
    "             inputs,\n",
    "             outputs,\n",
    "             num_samples):\n",
    "        \n",
    "        loss = 0\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            \n",
    "            N = np.random.choice(np.arange(1, inputs.shape[1]))\n",
    "            \n",
    "            ctx_in = inputs[:, :N]\n",
    "            ctx_out = outputs[:, :N]\n",
    "            trg_in = inputs[:, N:]\n",
    "            trg_out = outputs[:, N:]\n",
    "            \n",
    "            loss = loss + self._loss(ctx_in,\n",
    "                                     ctx_out,\n",
    "                                     trg_in,\n",
    "                                     trg_out)\n",
    "        \n",
    "        loss = loss / (num_samples * inputs.shape[1])\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "        \n",
    "        \n",
    "# =============================================================================\n",
    "# Fully Connected Translation Equivariant Encoder\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class FullyConnectedTEEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, deepset):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.deepset = deepset\n",
    "    \n",
    "    \n",
    "    def forward(self, x_ctx, y_ctx, x_trg):\n",
    "        \n",
    "        assert len(x_ctx.shape) == 3\n",
    "        assert len(y_ctx.shape) == 3\n",
    "        assert len(x_trg.shape) == 3\n",
    "        \n",
    "        # Compute context input pairwise differences\n",
    "        x_diff = x_ctx[:, None, :, :] - x_ctx[:, :, None, :]\n",
    "        \n",
    "        # Tile context outputs to concatenate with input differences\n",
    "        y_ctx_tile1 = y_ctx[:, None, :, :].repeat(1, x_diff.shape[1], 1, 1)\n",
    "        y_ctx_tile2 = y_ctx[:, :, None, :].repeat(1, 1, x_diff.shape[2], 1)\n",
    "        \n",
    "        # Concatenate input differences and outputs, to obtain complete context\n",
    "        ctx = torch.cat([x_diff, y_ctx_tile1, y_ctx_tile2], dim=-1)\n",
    "        \n",
    "        # Latent representation of context set -- resulting r has shape (B, C, R)\n",
    "        r = self.deepset(ctx)\n",
    "        \n",
    "        return r\n",
    "    \n",
    "\n",
    "# =============================================================================\n",
    "# Standard Translation Equivariant Encoder\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class StandardFullyConnectedTEEncoder(FullyConnectedTEEncoder):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 output_dim,\n",
    "                 rep_dim):\n",
    "        \n",
    "        # Input dimension of encoder (Din + 2 * Dout)\n",
    "        element_input_dim = input_dim + 2 * output_dim\n",
    "        \n",
    "        # Sizes of hidden layers and nonlinearity type\n",
    "        # Used for both elementwise and aggregate networks\n",
    "        hidden_dims = [128, 128]\n",
    "        nonlinearity = 'ReLU'\n",
    "        \n",
    "        # Element network -- in (B, C, C, Din + 2 * Dout), out (B, C, C, R)\n",
    "        element_network = FullyConnectedNetwork(input_dim=element_input_dim,\n",
    "                                                output_dim=rep_dim,\n",
    "                                                hidden_dims=hidden_dims,\n",
    "                                                nonlinearity=nonlinearity)\n",
    "        \n",
    "        # Dimensions to mean over -- in (B, C, C, R), out (B, R)\n",
    "        aggregation_dims = [1]\n",
    "        \n",
    "        # Aggregate network -- in (B, R), out (B, R)\n",
    "        aggregate_network = FullyConnectedNetwork(input_dim=rep_dim,\n",
    "                                                  output_dim=rep_dim,\n",
    "                                                  hidden_dims=hidden_dims,\n",
    "                                                  nonlinearity=nonlinearity)\n",
    "        \n",
    "        # Deepset architecture\n",
    "        deepset = FullyConnectedDeepSet(element_network,\n",
    "                                        aggregation_dims,\n",
    "                                        aggregate_network)\n",
    "        \n",
    "        super().__init__(deepset=deepset)\n",
    "        \n",
    "        \n",
    "        \n",
    "# =============================================================================\n",
    "# Fully Connected Translation Equivariant Decoder\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class FullyConnectedTEDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "    \n",
    "    \n",
    "    def forward(self, r, x_ctx, y_ctx, x_trg):\n",
    "        \n",
    "        \"\"\"\n",
    "        r     : (B, C, R)\n",
    "        x_ctx : (B, C, Din)\n",
    "        y_ctx : (B, C, Dout)\n",
    "        x_ctx : (B, T, Din)\n",
    "        \"\"\"\n",
    "\n",
    "        D = y_ctx.shape[-1]\n",
    "        \n",
    "        # Context and target inputs\n",
    "        x_ctx = x_ctx[:, :, None, :]\n",
    "        x_trg = x_trg[:, None, :, :]\n",
    "        \n",
    "        diff = x_ctx - x_trg\n",
    "        \n",
    "        y_ctx = y_ctx[:, :, None, :]\n",
    "        y_ctx = y_ctx.repeat(1, 1, diff.shape[2], 1)\n",
    "        \n",
    "        r = r[:, :, None, :].repeat(1, 1, diff.shape[2], 1)\n",
    "        \n",
    "        ctx = torch.cat([diff, r, y_ctx], dim=-1) # (B, C, T, Din + R + Dout)\n",
    "        \n",
    "        tensor = self.deepset(ctx)\n",
    "        \n",
    "        return tensor\n",
    "        \n",
    "        \n",
    "\n",
    "# =============================================================================\n",
    "# Standard Translation Equivariant Decoder\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class StandardFullyConnectedTEDecoder(FullyConnectedTEDecoder):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 output_dim,\n",
    "                 rep_dim,\n",
    "                 embedding_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Input dimension of encoder (Din + R)\n",
    "        element_input_dim = input_dim + output_dim + rep_dim\n",
    "        \n",
    "        # Sizes of hidden layers and nonlinearity type\n",
    "        # Used for both elementwise and aggregate networks\n",
    "        hidden_dims = [128, 128]\n",
    "        nonlinearity = 'ReLU'\n",
    "        \n",
    "        # Element network -- in (B, C, T, Din + R), out (B, C, T, R)\n",
    "        element_network = FullyConnectedNetwork(input_dim=element_input_dim,\n",
    "                                                output_dim=rep_dim,\n",
    "                                                hidden_dims=hidden_dims,\n",
    "                                                nonlinearity=nonlinearity)\n",
    "        \n",
    "        # Dimensions to mean over -- in (B, C, T, R), out (B, T, R)\n",
    "        aggregation_dims = [1]\n",
    "        \n",
    "        # Aggregate network -- in (B, T, R), out (B, T, E)\n",
    "        aggregate_network = FullyConnectedNetwork(input_dim=rep_dim,\n",
    "                                                  output_dim=embedding_dim,\n",
    "                                                  hidden_dims=hidden_dims,\n",
    "                                                  nonlinearity=nonlinearity)\n",
    "        \n",
    "        # Deepset architecture\n",
    "        deepset = FullyConnectedDeepSet(element_network,\n",
    "                                        aggregation_dims,\n",
    "                                        aggregate_network)\n",
    "        \n",
    "        self.deepset = deepset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "input_dim = 1\n",
    "output_dim = 1\n",
    "context_rep_features = 128\n",
    "embedding_features = 512\n",
    "log_noise = 0.\n",
    "\n",
    "encoder_input_dim = input_dim + output_dim\n",
    "encoder_output_dim = context_rep_features\n",
    "encoder_hidden_dims = [128, 128]\n",
    "\n",
    "decoder_input_dim = context_rep_features\n",
    "decoder_output_dim = output_dim + embedding_features\n",
    "decoder_hidden_dims = [128, 128]\n",
    "nonlinearity = 'ReLU'\n",
    "\n",
    "# encoder_fcn = FullyConnectedNetwork(input_dim=encoder_input_dim,\n",
    "#                                     output_dim=encoder_output_dim,\n",
    "#                                     hidden_dims=encoder_hidden_dims,\n",
    "#                                     nonlinearity=nonlinearity)\n",
    "\n",
    "# decoder_fcn = FullyConnectedNetwork(input_dim=decoder_input_dim,\n",
    "#                                     output_dim=decoder_output_dim,\n",
    "#                                     hidden_dims=decoder_hidden_dims,\n",
    "#                                     nonlinearity=nonlinearity)\n",
    "\n",
    "# fc_gnp = TranslationEquivariantGaussianNeuralProcess(encoder=encoder_fcn,\n",
    "#                                                      decoder=decoder_fcn,\n",
    "#                                                      log_noise=log_noise)\n",
    "\n",
    "num_basis_dim = 512\n",
    "\n",
    "\n",
    "covariance = InnerProdCov(num_basis_dim)\n",
    "add_noise = AddHomoNoise()\n",
    "\n",
    "fc_gnp = StandardFullyConnectedTEGNP(covariance=covariance, add_noise=add_noise)\n",
    "\n",
    "\n",
    "# Dataset parameters\n",
    "xmin = - 3e0\n",
    "xmax = 3e0\n",
    "num_batches = 16\n",
    "batch_size = 32\n",
    "plot_batch_size = 32\n",
    "scale = 1e0\n",
    "cov_coeff = 1e0\n",
    "noise_coeff = 1e-1\n",
    "num_samples = 2\n",
    "\n",
    "# Training parameters and optimizer\n",
    "num_train_steps = int(1e5)\n",
    "lr = 1e-3\n",
    "\n",
    "optimizer = torch.optim.Adam(fc_gnp.parameters(), lr=lr)\n",
    "\n",
    "losses = []\n",
    "\n",
    "for step in range(num_train_steps):\n",
    "    \n",
    "    inputs, outputs = sample_datasets_from_gps(xmin,\n",
    "                                               xmax,\n",
    "                                               num_batches,\n",
    "                                               batch_size,\n",
    "                                               scale,\n",
    "                                               cov_coeff,\n",
    "                                               noise_coeff,\n",
    "                                               as_tensor=True)\n",
    "    \n",
    "    loss = fc_gnp.loss(inputs, outputs, num_samples=num_samples)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % int(1e2) == 0:\n",
    "        \n",
    "        print(loss)\n",
    "        \n",
    "        plot_samples_and_predictions(fc_gnp,\n",
    "                                     xmin,\n",
    "                                     xmax,\n",
    "                                     plot_batch_size,\n",
    "                                     scale,\n",
    "                                     cov_coeff,\n",
    "                                     noise_coeff,\n",
    "                                     step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-kernelcnp",
   "language": "python",
   "name": "venv-kernelcnp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
