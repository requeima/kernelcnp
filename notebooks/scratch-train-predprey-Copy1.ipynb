{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7894e7d-e9a6-4bbd-9eea-ce830e23c125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from cnp.experiment import WorkingDirectory\n",
    "\n",
    "from cnp.cnp import GaussianNeuralProcess, StandardConvGNP\n",
    "from cnp.lnp import StandardConvNP\n",
    "from cnp.architectures import UNet\n",
    "\n",
    "from cnp.cov import (\n",
    "    OutputLayer,\n",
    "    MeanFieldGaussianLayer,\n",
    "    InnerprodGaussianLayer,\n",
    "    KvvGaussianLayer,\n",
    "    LogLogitCopulaLayer,\n",
    "    ExponentialCopulaLayer\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "from cnp.encoders import (\n",
    "    StandardEncoder,\n",
    "    ConvEncoder,\n",
    "    ConvPDEncoder,\n",
    ")\n",
    "\n",
    "from cnp.decoders import (\n",
    "    StandardDecoder,\n",
    "    ConvDecoder,\n",
    "    ConvPDDecoder,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.distributions import (\n",
    "    Normal,\n",
    "    MultivariateNormal,\n",
    "    LowRankMultivariateNormal\n",
    ")\n",
    "\n",
    "from cnp.cov import GaussianLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a7e6ad4-a12e-4585-984d-dbfb5cdfdee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_data = 'sim-pred-prey-16-50-100-100-0'\n",
    "args_cov_type = 'meanfield'\n",
    "args_noise_type = 'homo'\n",
    "args_marginal_type = 'exponential'\n",
    "args_model = 'convGNP'\n",
    "args_num_basis_dim = 32\n",
    "\n",
    "args_seed = 0\n",
    "args_learning_rate = 5e-4\n",
    "args_weight_decay = 0.\n",
    "args_validate_every = 1\n",
    "args_jitter = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffebba7d-8609-476d-8581-a9c3cf8abfdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root: /scratches/cblgpu07/em626/kernelcnp/kernelcnp/experiments/predator-prey/results/sim-pred-prey-16-50-100-100-0/models/convGNP/meanfield/homo/exponential/seed-0\n",
      "Root: /scratches/cblgpu07/em626/kernelcnp/kernelcnp/experiments/predator-prey/simulated-data/sim-pred-prey-16-50-100-100-0\n"
     ]
    }
   ],
   "source": [
    "# Set seed\n",
    "np.random.seed(args_seed)\n",
    "torch.manual_seed(args_seed)\n",
    "\n",
    "# root = '/Users/stratis/repos/kernelcnp/kernelcnp/experiments/predator-prey'\n",
    "root = '/scratches/cblgpu07/em626/kernelcnp/kernelcnp/experiments/predator-prey'\n",
    "\n",
    "# Working directory for saving results\n",
    "experiment_name = os.path.join(f'{root}',\n",
    "                               f'results',\n",
    "                               f'{args_data}',\n",
    "                               f'models',\n",
    "                               f'{args_model}',\n",
    "                               f'{args_cov_type}',\n",
    "                               f'{args_noise_type}',\n",
    "                               f'{args_marginal_type}',\n",
    "                               f'seed-{args_seed}')\n",
    "working_directory = WorkingDirectory(root=experiment_name)\n",
    "\n",
    "# Data directory for loading data\n",
    "data_root = os.path.join(f'{root}',\n",
    "                         f'simulated-data',\n",
    "                         f'{args_data}')\n",
    "data_directory = WorkingDirectory(root=data_root)\n",
    "    \n",
    "file = open(working_directory.file('data_location.txt'), 'w')\n",
    "file.write(data_directory.root)\n",
    "file.close()\n",
    "\n",
    "# =============================================================================\n",
    "# Load data and validation oracle generator\n",
    "# =============================================================================\n",
    "    \n",
    "file = open(data_directory.file('train-data.pkl'), 'rb')\n",
    "data_train = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open(data_directory.file('valid-data.pkl'), 'rb')\n",
    "data_val = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17769b3d-743f-48b4-b721-59ffaa57cd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Training epoch helper\n",
    "# =============================================================================\n",
    "\n",
    "def train(data,\n",
    "          model,\n",
    "          optimiser,\n",
    "          log_every,\n",
    "          device,\n",
    "          writer,\n",
    "          iteration):\n",
    "    \n",
    "    for step, batch in enumerate(data):\n",
    "\n",
    "        nll = model.loss(batch['x_context'][:, :, None].to(device),\n",
    "                         batch['y_context'][:, 0, :, None].to(device) / 100 + 1e-2,\n",
    "                         batch['x_target'][:, :, None].to(device),\n",
    "                         batch['y_target'][:, 0, :, None].to(device) / 100 + 1e-2)\n",
    "        \n",
    "        encoder_scale = torch.exp(model.encoder.sigma).detach().cpu().numpy().squeeze()\n",
    "        decoder_scale = torch.exp(model.decoder.sigma).detach().cpu().numpy().squeeze()\n",
    "\n",
    "        if step % log_every == 0:\n",
    "            print(f\"Training   neg. log-lik: {nll:.2f}, \"\n",
    "                  f\"Encoder/decoder scales {encoder_scale:.3f} \"\n",
    "                  f\"{decoder_scale:.3f}\")\n",
    "\n",
    "        # Compute gradients and apply them\n",
    "        nll.backward()\n",
    "        optimiser.step()\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "#         # Write to tensorboard\n",
    "#         writer.add_scalar('Train log-lik.', - nll, iteration)\n",
    "        \n",
    "        iteration = iteration + 1\n",
    "        \n",
    "    return iteration\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Validation helper\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def validate(data,\n",
    "             model,\n",
    "             device,\n",
    "             writer,\n",
    "             latent_model):\n",
    "    \n",
    "    # Lists for logging model's training NLL and oracle NLL\n",
    "    nll_list = []\n",
    "    oracle_nll_list = []\n",
    "    \n",
    "    # If training a latent model, set the number of latent samples accordingly\n",
    "    loss_kwargs = {'num_samples' : args_np_val_samples} \\\n",
    "                  if latent_model else {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(data):\n",
    "            \n",
    "            print(batch['x_context'].shape,\n",
    "                  batch['y_context'].shape,\n",
    "                  batch['x_target'].shape,\n",
    "                  batch['y_target'].shape)\n",
    "            \n",
    "            nll = model.loss(batch['x_context'][:, :, None].to(device),\n",
    "                             batch['y_context'][:, 0, :, None].to(device) / 100 + 1e-2,\n",
    "                             batch['x_target'][:, :, None].to(device),\n",
    "                             batch['y_target'][:, 0, :, None].to(device) / 100 + 1e-2,\n",
    "                             **loss_kwargs)\n",
    "            \n",
    "            # Scale by the average number of target points\n",
    "            nll_list.append(nll.item())\n",
    "\n",
    "    mean_nll = np.mean(nll_list)\n",
    "    std_nll = np.var(nll_list)**0.5\n",
    "\n",
    "    # Print validation loss and oracle loss\n",
    "    print(f\"Validation neg. log-lik: \"\n",
    "          f\"{mean_nll:.2f}\")\n",
    "\n",
    "    return mean_nll, std_nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab4e857f-99a9-481e-803a-7b4d9b2d209a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardConvGNP(GaussianNeuralProcess):\n",
    "    \n",
    "    def __init__(self, input_dim, output_layer):\n",
    "        \n",
    "        # Standard input/output dimensions and discretisation density\n",
    "        output_dim = 1\n",
    "        points_per_unit = 64\n",
    "\n",
    "        conv_channels = 8\n",
    "        conv_in_channels = conv_channels\n",
    "        conv_out_channels = 8\n",
    "        \n",
    "        # Standard convolutional architecture\n",
    "        conv_architecture = UNet(input_dim=input_dim,\n",
    "                                 in_channels=conv_in_channels,\n",
    "                                 out_channels=conv_out_channels)\n",
    "\n",
    "        # Construct the convolutional encoder\n",
    "        grid_multiplyer =  2 ** conv_architecture.num_halving_layers\n",
    "        encoder_init_length_scale = 1e-1 # 1.0 / points_per_unit\n",
    "        decoder_init_length_scale = 1e-1 # 1.0 / points_per_unit\n",
    "        grid_margin = 5.\n",
    "        \n",
    "        encoder = ConvEncoder(input_dim=input_dim,\n",
    "                              out_channels=conv_channels,\n",
    "                              init_length_scale=encoder_init_length_scale,\n",
    "                              points_per_unit=points_per_unit,\n",
    "                              grid_multiplier=grid_multiplyer,\n",
    "                              grid_margin=grid_margin)\n",
    "        \n",
    "        # Construct the convolutional decoder\n",
    "        decoder_out_channels = output_layer.num_features\n",
    "        \n",
    "        decoder = ConvDecoder(input_dim=input_dim,\n",
    "                              conv_architecture=conv_architecture,\n",
    "                              conv_out_channels=conv_architecture.out_channels,\n",
    "                              out_channels=decoder_out_channels,\n",
    "                              init_length_scale=decoder_init_length_scale,\n",
    "                              points_per_unit=points_per_unit,\n",
    "                              grid_multiplier=grid_multiplyer,\n",
    "                              grid_margin=grid_margin)\n",
    "\n",
    "\n",
    "        super().__init__(encoder=encoder,\n",
    "                         decoder=decoder,\n",
    "                         output_layer=output_layer)\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.conv_architecture = conv_architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "097e070a-e18d-4e05-a016-4546bf82d741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim-pred-prey-16-50-100-100-0 convGNP meanfield homo exponential 32: 50711.0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Create model\n",
    "# =============================================================================\n",
    "    \n",
    "# Set device\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(0)\n",
    "    \n",
    "use_cpu = False\n",
    "device = torch.device('cpu') if use_cpu else torch.device('cuda')\n",
    "\n",
    "cov_types = {\n",
    "    'meanfield' : MeanFieldGaussianLayer,\n",
    "    'innerprod' : InnerprodGaussianLayer,\n",
    "    'kvv'       : KvvGaussianLayer\n",
    "}\n",
    "\n",
    "if args_cov_type == 'meanfield':\n",
    "    output_layer = MeanFieldGaussianLayer(jitter=args_jitter,\n",
    "                                          constrain_variance=False)\n",
    "    \n",
    "else:\n",
    "    output_layer = cov_types[args_cov_type](num_embedding=args_num_basis_dim,\n",
    "                                            noise_type=args_noise_type,\n",
    "                                            jitter=args_jitter)\n",
    "\n",
    "if args_marginal_type == 'loglogit':\n",
    "    output_layer = LogLogitCopulaLayer(gaussian_layer=output_layer,\n",
    "                                       device=device)\n",
    "if args_marginal_type == 'exponential':\n",
    "    output_layer = ExponentialCopulaLayer(gaussian_layer=output_layer,\n",
    "                                          device=device)\n",
    "    \n",
    "# Create model architecture\n",
    "if args_model == 'convGNP':\n",
    "    model = StandardConvGNP(input_dim=1, output_layer=output_layer)\n",
    "    \n",
    "elif args_model == 'convNP':\n",
    "    model = StandardConvNP(input_dim=1,\n",
    "                           num_samples=args_np_loss_samples)\n",
    "    \n",
    "else:\n",
    "    raise ValueError(f'Unknown model {args_model}.')\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "latent_model = args_model == 'convNP'\n",
    "\n",
    "print(f'{args_data} '\n",
    "      f'{args_model} '\n",
    "      f'{args_cov_type} '\n",
    "      f'{args_noise_type} '\n",
    "      f'{args_marginal_type} '\n",
    "      f'{args_num_basis_dim}: '\n",
    "      f'{model.num_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32208b30-5987-41dc-9d66-f6baf7859968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1/101\n",
      "Training   neg. log-lik: 320.86, Encoder/decoder scales 0.100 0.100\n",
      "Training   neg. log-lik: 118.82, Encoder/decoder scales 0.099 0.098\n",
      "Training   neg. log-lik: 122.75, Encoder/decoder scales 0.098 0.096\n",
      "Training   neg. log-lik: 167.75, Encoder/decoder scales 0.098 0.096\n",
      "Training   neg. log-lik: 160.76, Encoder/decoder scales 0.097 0.096\n",
      "Training   neg. log-lik: 187.29, Encoder/decoder scales 0.096 0.096\n",
      "Training   neg. log-lik: 155.67, Encoder/decoder scales 0.095 0.096\n",
      "Training   neg. log-lik: 205.71, Encoder/decoder scales 0.095 0.096\n",
      "Training   neg. log-lik: 319.28, Encoder/decoder scales 0.096 0.095\n",
      "Training   neg. log-lik: 300.89, Encoder/decoder scales 0.096 0.095\n",
      "Training   neg. log-lik: 232.69, Encoder/decoder scales 0.096 0.095\n",
      "\n",
      "Epoch: 2/101\n",
      "Training   neg. log-lik: 251.58, Encoder/decoder scales 0.096 0.095\n",
      "Training   neg. log-lik: 218.83, Encoder/decoder scales 0.096 0.094\n",
      "Training   neg. log-lik: 255.87, Encoder/decoder scales 0.096 0.094\n",
      "Training   neg. log-lik: 210.73, Encoder/decoder scales 0.096 0.094\n",
      "Training   neg. log-lik: 178.09, Encoder/decoder scales 0.095 0.094\n",
      "Training   neg. log-lik: 269.28, Encoder/decoder scales 0.095 0.094\n",
      "Training   neg. log-lik: 223.45, Encoder/decoder scales 0.096 0.095\n",
      "Training   neg. log-lik: 139.98, Encoder/decoder scales 0.096 0.095\n",
      "Training   neg. log-lik: 189.48, Encoder/decoder scales 0.096 0.095\n",
      "Training   neg. log-lik: 101.06, Encoder/decoder scales 0.096 0.095\n",
      "Training   neg. log-lik: 293.01, Encoder/decoder scales 0.096 0.094\n",
      "\n",
      "Epoch: 3/101\n",
      "Training   neg. log-lik: 233.17, Encoder/decoder scales 0.096 0.094\n",
      "Training   neg. log-lik: 271.75, Encoder/decoder scales 0.096 0.094\n",
      "Training   neg. log-lik: 304.34, Encoder/decoder scales 0.096 0.094\n",
      "Training   neg. log-lik: 253.95, Encoder/decoder scales 0.096 0.094\n",
      "Training   neg. log-lik: 227.93, Encoder/decoder scales 0.096 0.094\n",
      "Training   neg. log-lik: 240.21, Encoder/decoder scales 0.096 0.094\n",
      "Training   neg. log-lik: 227.80, Encoder/decoder scales 0.096 0.094\n",
      "Training   neg. log-lik: 224.72, Encoder/decoder scales 0.096 0.094\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The parameter loc has invalid values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-002821bec37c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m                             \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                             train_iteration)\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m#     save_checkpoint(working_directory,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-75f0f2798362>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(data, model, optimiser, log_every, device, writer, iteration)\u001b[0m\n\u001b[1;32m     16\u001b[0m                          \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_context'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                          \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x_target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                          batch['y_target'][:, 0, :, None].to(device) / 100 + 1e-2)\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mencoder_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratches/cblgpu07/em626/kernelcnp/kernelcnp/cnp/cnp.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x_context, y_context, x_target, y_target, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mloglik\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloglik\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mnll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloglik\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratches/cblgpu07/em626/kernelcnp/kernelcnp/cnp/cov.py\u001b[0m in \u001b[0;36mloglik\u001b[0;34m(self, tensor, y_target)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0;31m# Log-likelihood of transformed variables under Gaussian\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m         \u001b[0mloglik\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgaussian_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloglik\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_target\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mv_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;31m# Compute change-of-variables contribution (Jacobian is diagonal)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratches/cblgpu07/em626/kernelcnp/kernelcnp/cnp/cov.py\u001b[0m in \u001b[0;36mloglik\u001b[0;34m(self, tensor, y_target, double)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;31m# Initialise distribution and compute log probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoiseless\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdouble\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0mloglik\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratches/cblgpu07/em626/kernelcnp/kernelcnp/cnp/cov.py\u001b[0m in \u001b[0;36mdistribution\u001b[0;34m(self, tensor, noiseless, double)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# Create distribution and return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultivariateNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_tril\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscale_tril\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratches/cblgpu07/em626/kernelcnp/venv-gnp/lib/python3.6/site-packages/torch/distributions/multivariate_normal.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loc, covariance_matrix, precision_matrix, scale_tril, validate_args)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mevent_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMultivariateNormal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mscale_tril\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratches/cblgpu07/em626/kernelcnp/venv-gnp/lib/python3.6/site-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     51\u001b[0m                     \u001b[0;32mcontinue\u001b[0m  \u001b[0;31m# skip checking lazily-constructed args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconstraint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The parameter {} has invalid values\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDistribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The parameter loc has invalid values"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Train or test model\n",
    "# =============================================================================\n",
    "\n",
    "# Number of epochs between validations\n",
    "train_iteration = 0\n",
    "log_every = 100\n",
    "\n",
    "# Create optimiser\n",
    "optimiser = torch.optim.Adam(model.parameters(),\n",
    "                         args_learning_rate,\n",
    "                         weight_decay=args_weight_decay)\n",
    "\n",
    "# Run the training loop, maintaining the best objective value\n",
    "best_nll = np.inf\n",
    "\n",
    "epochs = len(data_train)\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    print('\\nEpoch: {}/{}'.format(epoch+1, epochs))\n",
    "\n",
    "    if False: # epoch % args_validate_every == 0:\n",
    "\n",
    "        valid_epoch = data_val[epoch // args_validate_every]\n",
    "\n",
    "        # Compute negative log-likelihood on validation data\n",
    "        val_nll, _,  = validate(valid_epoch,\n",
    "                                model,\n",
    "                                device,\n",
    "                                None,\n",
    "                                latent_model)\n",
    "\n",
    "#         # Log information to tensorboard\n",
    "#         writer.add_scalar('True data log-lik.',\n",
    "#                           -true_nll,\n",
    "#                           epoch)\n",
    "\n",
    "#         # Log information to tensorboard\n",
    "#         writer.add_scalar('Validation log-lik.',\n",
    "#                           -val_nll,\n",
    "#                           epoch)\n",
    "\n",
    "        # Update the best objective value and checkpoint the model\n",
    "        is_best, best_obj = (True, val_nll) if val_nll < best_nll else \\\n",
    "                            (False, best_nll)\n",
    "\n",
    "\n",
    "    train_epoch = data_train[epoch]\n",
    "\n",
    "    # Compute training negative log-likelihood\n",
    "    train_iteration = train(train_epoch,\n",
    "                            model,\n",
    "                            optimiser,\n",
    "                            log_every,\n",
    "                            device,\n",
    "                            None,\n",
    "                            train_iteration)\n",
    "\n",
    "#     save_checkpoint(working_directory,\n",
    "#                     {'epoch'         : epoch + 1,\n",
    "#                      'state_dict'    : model.state_dict(),\n",
    "#                      'best_acc_top1' : best_obj,\n",
    "#                      'optimizer'     : optimiser.state_dict()},\n",
    "#                     is_best=is_best,\n",
    "#                     epoch=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74dc2b5-2c4b-46ae-8ea4-983c6c21adab",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_idx = 99\n",
    "i_idx = 11\n",
    "b_idx = 0\n",
    "\n",
    "train_epoch = data_train[e_idx]\n",
    "\n",
    "x_min = torch.min(train_epoch[i_idx]['x_target'][b_idx, :])\n",
    "x_max = torch.max(train_epoch[i_idx]['x_target'][b_idx, :])\n",
    "x_plot = torch.linspace(x_min-3., x_max+3., 500)[None, :, None].to(device)\n",
    "x_plot = x_plot.repeat(train_epoch[i_idx]['x_context'].shape[0], 1, 1).to(device)\n",
    "\n",
    "max_ctx = train_epoch[i_idx]['x_context'].shape[1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    samples = [model.sample(train_epoch[i_idx]['x_context'][:, :, None].to(device),\n",
    "                            train_epoch[i_idx]['y_context'][:, 0, :, None].to(device) / 100 + 1e-2,\n",
    "                            x_plot,\n",
    "                            num_samples=100,\n",
    "                            noiseless=False,\n",
    "                            double=True)\n",
    "               for i in range(2)]\n",
    "    samples = torch.cat(samples, dim=0)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x_plot[0, :, 0].cpu().detach().numpy(),\n",
    "         samples[:, b_idx, :].cpu().detach().numpy().T,\n",
    "         color='green',\n",
    "         alpha=0.01,\n",
    "         zorder=1)\n",
    "\n",
    "plt.plot(x_plot[0, :, 0].cpu().detach().numpy(),\n",
    "         torch.mean(samples[:, b_idx, :], axis=0).cpu().detach().numpy().T,\n",
    "         color='black',\n",
    "         alpha=0.5,\n",
    "         zorder=2)\n",
    "\n",
    "plt.scatter(train_epoch[i_idx]['x_context'][b_idx, :].detach().numpy(),\n",
    "            train_epoch[i_idx]['y_context'][b_idx, 0, :].detach().numpy() / 100,\n",
    "            color='black',\n",
    "            zorder=2)\n",
    "\n",
    "# plt.scatter(train_epoch[i_idx]['x_context'][b_idx, max_ctx:].detach().numpy(),\n",
    "#             train_epoch[i_idx]['y_context'][b_idx, 0, max_ctx:].detach().numpy() / 100,\n",
    "#             color='red',\n",
    "#             zorder=2)\n",
    "\n",
    "plt.scatter(train_epoch[i_idx]['x_target'][b_idx, :].detach().numpy(),\n",
    "            train_epoch[i_idx]['y_target'][b_idx, 0, :].detach().numpy() / 100,\n",
    "            color='red',\n",
    "            zorder=2)\n",
    "\n",
    "plt.ylim([0, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f744c42-a03f-4ac6-b1be-581d98b8fc9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476659aa-7ecd-4717-af77-9c86e6e79b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-gnp",
   "language": "python",
   "name": "venv-gnp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
