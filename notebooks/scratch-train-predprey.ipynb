{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7894e7d-e9a6-4bbd-9eea-ce830e23c125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from cnp.experiment import WorkingDirectory\n",
    "\n",
    "from cnp.cnp import GaussianNeuralProcess, StandardConvGNP\n",
    "from cnp.lnp import StandardConvNP\n",
    "from cnp.architectures import UNet\n",
    "\n",
    "from cnp.cov import (\n",
    "    OutputLayer,\n",
    "    MeanFieldGaussianLayer,\n",
    "    InnerprodGaussianLayer,\n",
    "    KvvGaussianLayer,\n",
    "    LogLogitCopulaLayer\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "from cnp.encoders import (\n",
    "    StandardEncoder,\n",
    "    ConvEncoder,\n",
    "    ConvPDEncoder,\n",
    ")\n",
    "\n",
    "from cnp.decoders import (\n",
    "    StandardDecoder,\n",
    "    ConvDecoder,\n",
    "    ConvPDDecoder,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.distributions import (\n",
    "    Normal,\n",
    "    MultivariateNormal,\n",
    "    LowRankMultivariateNormal\n",
    ")\n",
    "\n",
    "from cnp.cov import GaussianLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7e6ad4-a12e-4585-984d-dbfb5cdfdee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_data = 'sim-pred-prey-16-50-100-100-0' # 'sim-pred-prey-16-50-100-100-0'\n",
    "args_cov_type = 'meanfield'\n",
    "args_noise_type = 'homo'\n",
    "args_marginal_type = 'loglogit'\n",
    "args_model = 'convGNP'\n",
    "args_num_basis_dim = 32\n",
    "\n",
    "args_seed = 0\n",
    "args_learning_rate = 1e-4\n",
    "args_weight_decay = 0.\n",
    "args_validate_every = 1\n",
    "args_jitter = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffebba7d-8609-476d-8581-a9c3cf8abfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "np.random.seed(args_seed)\n",
    "torch.manual_seed(args_seed)\n",
    "\n",
    "# root = '/Users/stratis/repos/kernelcnp/kernelcnp/experiments/predator-prey'\n",
    "root = '/scratches/cblgpu07/em626/kernelcnp/kernelcnp/experiments/predator-prey'\n",
    "\n",
    "# Working directory for saving results\n",
    "experiment_name = os.path.join(f'{root}',\n",
    "                               f'results',\n",
    "                               f'{args_data}',\n",
    "                               f'models',\n",
    "                               f'{args_model}',\n",
    "                               f'{args_cov_type}',\n",
    "                               f'{args_noise_type}',\n",
    "                               f'{args_marginal_type}',\n",
    "                               f'seed-{args_seed}')\n",
    "working_directory = WorkingDirectory(root=experiment_name)\n",
    "\n",
    "# Data directory for loading data\n",
    "data_root = os.path.join(f'{root}',\n",
    "                         f'simulated-data',\n",
    "                         f'{args_data}')\n",
    "data_directory = WorkingDirectory(root=data_root)\n",
    "    \n",
    "file = open(working_directory.file('data_location.txt'), 'w')\n",
    "file.write(data_directory.root)\n",
    "file.close()\n",
    "\n",
    "# =============================================================================\n",
    "# Load data and validation oracle generator\n",
    "# =============================================================================\n",
    "    \n",
    "file = open(data_directory.file('train-data.pkl'), 'rb')\n",
    "data_train = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open(data_directory.file('valid-data.pkl'), 'rb')\n",
    "data_val = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17769b3d-743f-48b4-b721-59ffaa57cd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Training epoch helper\n",
    "# =============================================================================\n",
    "\n",
    "def train(data,\n",
    "          model,\n",
    "          optimiser,\n",
    "          log_every,\n",
    "          device,\n",
    "          writer,\n",
    "          iteration):\n",
    "    \n",
    "    for step, batch in enumerate(data):\n",
    "\n",
    "        nll = model.loss(batch['x_context'][:, :, None].to(device),\n",
    "                         batch['y_context'][:, 0, :, None].to(device) / 100 + 1e-2,\n",
    "                         batch['x_target'][:, :, None].to(device),\n",
    "                         batch['y_target'][:, 0, :, None].to(device) / 100 + 1e-2)\n",
    "        \n",
    "        encoder_scale = torch.exp(model.encoder.sigma).detach().cpu().numpy().squeeze()\n",
    "        decoder_scale = torch.exp(model.decoder.sigma).detach().cpu().numpy().squeeze()\n",
    "\n",
    "        if step % log_every == 0:\n",
    "            print(f\"Training   neg. log-lik: {nll:.2f}, \"\n",
    "                  f\"Encoder/decoder scales {encoder_scale:.3f} \"\n",
    "                  f\"{decoder_scale:.3f}\")\n",
    "\n",
    "        # Compute gradients and apply them\n",
    "        nll.backward()\n",
    "        optimiser.step()\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "#         # Write to tensorboard\n",
    "#         writer.add_scalar('Train log-lik.', - nll, iteration)\n",
    "        \n",
    "        iteration = iteration + 1\n",
    "        \n",
    "    return iteration\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Validation helper\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def validate(data,\n",
    "             model,\n",
    "             device,\n",
    "             writer,\n",
    "             latent_model):\n",
    "    \n",
    "    # Lists for logging model's training NLL and oracle NLL\n",
    "    nll_list = []\n",
    "    oracle_nll_list = []\n",
    "    \n",
    "    # If training a latent model, set the number of latent samples accordingly\n",
    "    loss_kwargs = {'num_samples' : args_np_val_samples} \\\n",
    "                  if latent_model else {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(data):\n",
    "            \n",
    "            print(batch['x_context'].shape,\n",
    "                  batch['y_context'].shape,\n",
    "                  batch['x_target'].shape,\n",
    "                  batch['y_target'].shape)\n",
    "            \n",
    "            nll = model.loss(batch['x_context'][:, :, None].to(device),\n",
    "                             batch['y_context'][:, 0, :, None].to(device) / 100 + 1e-2,\n",
    "                             batch['x_target'][:, :, None].to(device),\n",
    "                             batch['y_target'][:, 0, :, None].to(device) / 100 + 1e-2,\n",
    "                             **loss_kwargs)\n",
    "            \n",
    "            # Scale by the average number of target points\n",
    "            nll_list.append(nll.item())\n",
    "\n",
    "    mean_nll = np.mean(nll_list)\n",
    "    std_nll = np.var(nll_list)**0.5\n",
    "\n",
    "    # Print validation loss and oracle loss\n",
    "    print(f\"Validation neg. log-lik: \"\n",
    "          f\"{mean_nll:.2f}\")\n",
    "\n",
    "    return mean_nll, std_nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4e857f-99a9-481e-803a-7b4d9b2d209a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardConvGNP(GaussianNeuralProcess):\n",
    "    \n",
    "    def __init__(self, input_dim, output_layer):\n",
    "        \n",
    "        # Standard input/output dimensions and discretisation density\n",
    "        output_dim = 1\n",
    "        points_per_unit = 64\n",
    "\n",
    "        conv_channels = 8\n",
    "        conv_in_channels = conv_channels\n",
    "        conv_out_channels = 8\n",
    "        \n",
    "        # Standard convolutional architecture\n",
    "        conv_architecture = UNet(input_dim=input_dim,\n",
    "                                 in_channels=conv_in_channels,\n",
    "                                 out_channels=conv_out_channels)\n",
    "\n",
    "        # Construct the convolutional encoder\n",
    "        grid_multiplyer =  2 ** conv_architecture.num_halving_layers\n",
    "        encoder_init_length_scale = 1e-1 # 1.0 / points_per_unit\n",
    "        decoder_init_length_scale = 1e-1 # 1.0 / points_per_unit\n",
    "        grid_margin = 5.\n",
    "        \n",
    "        encoder = ConvEncoder(input_dim=input_dim,\n",
    "                              out_channels=conv_channels,\n",
    "                              init_length_scale=encoder_init_length_scale,\n",
    "                              points_per_unit=points_per_unit,\n",
    "                              grid_multiplier=grid_multiplyer,\n",
    "                              grid_margin=grid_margin)\n",
    "        \n",
    "        # Construct the convolutional decoder\n",
    "        decoder_out_channels = output_layer.num_features\n",
    "        \n",
    "        decoder = ConvDecoder(input_dim=input_dim,\n",
    "                              conv_architecture=conv_architecture,\n",
    "                              conv_out_channels=conv_architecture.out_channels,\n",
    "                              out_channels=decoder_out_channels,\n",
    "                              init_length_scale=decoder_init_length_scale,\n",
    "                              points_per_unit=points_per_unit,\n",
    "                              grid_multiplier=grid_multiplyer,\n",
    "                              grid_margin=grid_margin)\n",
    "\n",
    "\n",
    "        super().__init__(encoder=encoder,\n",
    "                         decoder=decoder,\n",
    "                         output_layer=output_layer)\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.conv_architecture = conv_architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ad1893-618d-4e63-a382-1713489d9423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Log-logit copula output layer\n",
    "# =============================================================================\n",
    "\n",
    "class LogLogitCopulaLayer(OutputLayer):\n",
    "    \n",
    "    \n",
    "    def __init__(self, gaussian_layer, device):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Initialise Gaussian layer\n",
    "        self.gaussian_layer = gaussian_layer\n",
    "        \n",
    "        # Number of features equal to number of Gaussian layer features plus\n",
    "        # two additional features for the Gamma - rate and concentration\n",
    "        self.num_features = self.gaussian_layer.num_features + 2\n",
    "        \n",
    "        # Set device\n",
    "        self.device = device\n",
    "\n",
    "    \n",
    "    def loglik(self, tensor, y_target):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            tensor   : torch.tensor, (B, T, C)\n",
    "            y_target : torch.tensor, (B, T)\n",
    "            \n",
    "        Returns:\n",
    "            tensor : torch.tensor, (B, T)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Unpack parameters and apply inverse transformation\n",
    "        tensor, a, b = self.unpack_parameters(tensor=tensor)\n",
    "        v_target = self.inverse_marginal_transformation(x=y_target,\n",
    "                                                        a=a,\n",
    "                                                        b=b)\n",
    "        \n",
    "        # Log-likelihood of transformed variables under Gaussian\n",
    "        loglik = self.gaussian_layer.loglik(tensor=tensor, y_target=v_target)\n",
    "        \n",
    "        # Compute change-of-variables contribution (Jacobian is diagonal)\n",
    "        grad = self.inverse_marginal_transformation(x=y_target,\n",
    "                                                    a=a,\n",
    "                                                    b=b,\n",
    "                                                    grad=True)\n",
    "        jacobian_term = torch.sum(torch.log(torch.abs(grad)), dim=-1)\n",
    "        \n",
    "        # Ensure shapes are compatible\n",
    "        assert loglik.shape == jacobian_term.shape\n",
    "        \n",
    "        return loglik + jacobian_term\n",
    "\n",
    "    \n",
    "    def sample(self, tensor, num_samples, noiseless, double=False):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            tensor      : torch.tensor, (B, T, C)\n",
    "            num_samples : int, number of samples to draw\n",
    "            noiseless   : bool, whether to include the noise term\n",
    "            \n",
    "        Returns:\n",
    "            tensor : torch.tensor, (B, T)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Unpack parameters and apply inverse transformation\n",
    "        tensor, a, b = self.unpack_parameters(tensor=tensor)\n",
    "        \n",
    "        # Draw samples from Gaussian and apply marginal transformation\n",
    "        v_samples = self.gaussian_layer.sample(tensor=tensor,\n",
    "                                               num_samples=num_samples,\n",
    "                                               noiseless=noiseless,\n",
    "                                               double=double)\n",
    "        \n",
    "        # Repeat a and b, (num_samples, B, T)\n",
    "        a = a[None, :, :].repeat(num_samples, 1, 1)\n",
    "        b = b[None, :, :].repeat(num_samples, 1, 1)\n",
    "        \n",
    "        # Apply marginal transformation to Gaussian samples\n",
    "        samples = self.marginal_transformation(v_samples, a=a, b=b)\n",
    "        \n",
    "        return samples\n",
    "        \n",
    "        \n",
    "    def unpack_parameters(self, tensor):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            tensor : torch.tensor, (B, T, C)\n",
    "            \n",
    "        Returns:\n",
    "            tensor : torch.tensor, (B, T, C-2)\n",
    "            a      : torch.tensor, (B, T)\n",
    "            b      : torch.tensor, (B, T)\n",
    "        \"\"\"\n",
    "        \n",
    "        epsilon = 1e-2\n",
    "        \n",
    "        # Check tensor has correct number of features\n",
    "        assert (len(tensor.shape) == 3) and \\\n",
    "               (tensor.shape[-1] == self.num_features)\n",
    "        \n",
    "        # Get rate and concentration from tensor\n",
    "        a = 0. * tensor[:, :, 0] + 1. #torch.nn.Softplus()(tensor[:, :, 0]) + epsilon\n",
    "        b = torch.nn.Softplus()(1e-2 * tensor[:, :, 1]) + 1e0 + epsilon\n",
    "        \n",
    "        # Slice out rate and concentration\n",
    "        tensor = tensor[:, :, 2:] / 1e2\n",
    "        \n",
    "        return tensor, a, b\n",
    "    \n",
    "    \n",
    "    def pdf(self, x, a, b):\n",
    "        \"\"\"\n",
    "        Probability distribution function of the log-logistic distribution.\n",
    "        \n",
    "            PDF(x) = (b/a) * (x/a)^(b-1) / (1 + (x/a)^b)^2\n",
    "        \n",
    "        Arguments:\n",
    "            x : torch.tensor, (B, T)\n",
    "            a : torch.tensor, (B, T)\n",
    "            b : torch.tensor, (B, T)\n",
    "            \n",
    "        Returns:\n",
    "            tensor : torch.tensor, (B, T)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Check shapes are compatible, all x values are positive\n",
    "        assert x.shape == a.shape == b.shape\n",
    "        assert torch.all(x > 0.)\n",
    "        \n",
    "        return (b/a) * (x/a)**(b-1) / (1+(x/a)**b)**2\n",
    "    \n",
    "    \n",
    "    def cdf(self, x, a, b):\n",
    "        \"\"\"\n",
    "        Cumulative distribution function of the log-logistic distribution.\n",
    "        \n",
    "            CDF(x) = 1 / (1 + (x/a)^-b)\n",
    "        \n",
    "        Arguments:\n",
    "            x : torch.tensor, (B, T)\n",
    "            a : torch.tensor, (B, T)\n",
    "            b : torch.tensor, (B, T)\n",
    "            \n",
    "        Returns:\n",
    "            tensor : torch.tensor, (B, T)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Check shapes are compatible, all x values are positive\n",
    "        assert x.shape == a.shape == b.shape\n",
    "        assert torch.all(x > 0.)\n",
    "        \n",
    "        x = x.double()\n",
    "        a = a.double()\n",
    "        b = b.double()\n",
    "        \n",
    "        cdf = 1 / (1+(x/a)**-b)\n",
    "        cdf = cdf.float()\n",
    "        \n",
    "        return cdf\n",
    "    \n",
    "    \n",
    "    def icdf(self, x, a, b):\n",
    "        \"\"\"\n",
    "        Inverse cumulative distribution function of the log-logistic\n",
    "        distribution.\n",
    "        \n",
    "            CDF^-1(x) = a * (x^-1 - 1)^(-1/b)\n",
    "        \n",
    "        Arguments:\n",
    "            x : torch.tensor, (B, T)\n",
    "            a : torch.tensor, (B, T)\n",
    "            b : torch.tensor, (B, T)\n",
    "            \n",
    "        Returns:\n",
    "            tensor : torch.tensor, (B, T)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Check shapes are compatible, all x values are positive\n",
    "        assert x.shape == a.shape == b.shape\n",
    "        assert torch.all(x > 0.)\n",
    "        \n",
    "        x = x.double()\n",
    "        a = a.double()\n",
    "        b = b.double()\n",
    "        \n",
    "        icdf = a * (x**-1 - 1) ** (-1/b)\n",
    "        icdf = icdf.float()\n",
    "        \n",
    "        return icdf\n",
    "    \n",
    "    \n",
    "    def marginal_transformation(self, x, a, b):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x : torch.tensor, (B, T)\n",
    "            a : torch.tensor, (B, T)\n",
    "            b : torch.tensor, (B, T)\n",
    "            \n",
    "        Returns:\n",
    "            tensor : torch.tensor, (B, T)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Check shapes are compatible, all x values are positive\n",
    "        assert x.shape == a.shape == b.shape\n",
    "        \n",
    "        zeros = torch.zeros(size=x.shape).double().to(self.device)\n",
    "        ones = torch.ones(size=x.shape).double().to(self.device)\n",
    "        \n",
    "        gaussian = Normal(loc=zeros, scale=ones)\n",
    "        \n",
    "        x = gaussian.cdf(x)\n",
    "        x = self.icdf(x, a, b)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "    def inverse_marginal_transformation(self, x, a, b, grad=False):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x : torch.tensor, (B, T)\n",
    "            a : torch.tensor, (B, T)\n",
    "            b : torch.tensor, (B, T)\n",
    "            \n",
    "        Returns:\n",
    "            x : torch.tensor, (B, T)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Check shapes are compatible, all x values are positive\n",
    "        assert x.shape == a.shape == b.shape\n",
    "        assert torch.all(x > 0.)\n",
    "        \n",
    "        zeros = torch.zeros(size=x.shape).double().to(self.device)\n",
    "        ones = torch.ones(size=x.shape).double().to(self.device)\n",
    "        \n",
    "        gaussian = Normal(loc=zeros, scale=ones)\n",
    "        \n",
    "        if grad:\n",
    "            x = self.pdf(x, a, b) / gaussian.icdf(self.cdf(x, a, b))\n",
    "        \n",
    "        else:\n",
    "            x = self.cdf(x, a, b)\n",
    "            x = gaussian.icdf(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097e070a-e18d-4e05-a016-4546bf82d741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Create model\n",
    "# =============================================================================\n",
    "    \n",
    "# Set device\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(0)\n",
    "    \n",
    "use_cpu = False\n",
    "device = torch.device('cpu') if use_cpu else torch.device('cuda')\n",
    "\n",
    "cov_types = {\n",
    "    'meanfield' : MeanFieldGaussianLayer,\n",
    "    'innerprod' : InnerprodGaussianLayer,\n",
    "    'kvv'       : KvvGaussianLayer\n",
    "}\n",
    "\n",
    "if args_cov_type == 'meanfield':\n",
    "    output_layer = MeanFieldGaussianLayer(jitter=args_jitter)\n",
    "    \n",
    "else:\n",
    "    output_layer = cov_types[args_cov_type](num_embedding=args_num_basis_dim,\n",
    "                                            noise_type=args_noise_type,\n",
    "                                            jitter=args_jitter)\n",
    "\n",
    "if args_marginal_type == 'loglogit':\n",
    "    output_layer = LogLogitCopulaLayer(gaussian_layer=output_layer,\n",
    "                                       device=device)\n",
    "    \n",
    "# Create model architecture\n",
    "if args_model == 'convGNP':\n",
    "    model = StandardConvGNP(input_dim=1, output_layer=output_layer)\n",
    "    \n",
    "elif args_model == 'convNP':\n",
    "    model = StandardConvNP(input_dim=1,\n",
    "                           num_samples=args_np_loss_samples)\n",
    "    \n",
    "else:\n",
    "    raise ValueError(f'Unknown model {args_model}.')\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "latent_model = args_model == 'convNP'\n",
    "\n",
    "print(f'{args_data} '\n",
    "      f'{args_model} '\n",
    "      f'{args_cov_type} '\n",
    "      f'{args_noise_type} '\n",
    "      f'{args_marginal_type} '\n",
    "      f'{args_num_basis_dim}: '\n",
    "      f'{model.num_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32208b30-5987-41dc-9d66-f6baf7859968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Train or test model\n",
    "# =============================================================================\n",
    "\n",
    "# Number of epochs between validations\n",
    "train_iteration = 0\n",
    "log_every = 100\n",
    "\n",
    "# Create optimiser\n",
    "optimiser = torch.optim.Adam(model.parameters(),\n",
    "                         args_learning_rate,\n",
    "                         weight_decay=args_weight_decay)\n",
    "\n",
    "# Run the training loop, maintaining the best objective value\n",
    "best_nll = np.inf\n",
    "\n",
    "epochs = len(data_train)\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(30): # range(epochs):\n",
    "\n",
    "    print('\\nEpoch: {}/{}'.format(epoch+1, epochs))\n",
    "\n",
    "    if False: # epoch % args_validate_every == 0:\n",
    "\n",
    "        valid_epoch = data_val[epoch // args_validate_every]\n",
    "\n",
    "        # Compute negative log-likelihood on validation data\n",
    "        val_nll, _,  = validate(valid_epoch,\n",
    "                                model,\n",
    "                                device,\n",
    "                                None,\n",
    "                                latent_model)\n",
    "\n",
    "#         # Log information to tensorboard\n",
    "#         writer.add_scalar('True data log-lik.',\n",
    "#                           -true_nll,\n",
    "#                           epoch)\n",
    "\n",
    "#         # Log information to tensorboard\n",
    "#         writer.add_scalar('Validation log-lik.',\n",
    "#                           -val_nll,\n",
    "#                           epoch)\n",
    "\n",
    "        # Update the best objective value and checkpoint the model\n",
    "        is_best, best_obj = (True, val_nll) if val_nll < best_nll else \\\n",
    "                            (False, best_nll)\n",
    "\n",
    "\n",
    "    train_epoch = data_train[epoch]\n",
    "\n",
    "    # Compute training negative log-likelihood\n",
    "    train_iteration = train(train_epoch,\n",
    "                            model,\n",
    "                            optimiser,\n",
    "                            log_every,\n",
    "                            device,\n",
    "                            None,\n",
    "                            train_iteration)\n",
    "\n",
    "#     save_checkpoint(working_directory,\n",
    "#                     {'epoch'         : epoch + 1,\n",
    "#                      'state_dict'    : model.state_dict(),\n",
    "#                      'best_acc_top1' : best_obj,\n",
    "#                      'optimizer'     : optimiser.state_dict()},\n",
    "#                     is_best=is_best,\n",
    "#                     epoch=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74dc2b5-2c4b-46ae-8ea4-983c6c21adab",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_idx = 0\n",
    "i_idx = 3\n",
    "b_idx = 0\n",
    "\n",
    "train_epoch = data_train[e_idx]\n",
    "\n",
    "x_min = torch.min(train_epoch[i_idx]['x_target'][b_idx, :])\n",
    "x_max = torch.max(train_epoch[i_idx]['x_target'][b_idx, :])\n",
    "x_plot = torch.linspace(x_min-3., x_max+3., 500)[None, :, None].to(device)\n",
    "x_plot = x_plot.repeat(train_epoch[i_idx]['x_context'].shape[0], 1, 1).to(device)\n",
    "\n",
    "max_ctx = train_epoch[i_idx]['x_context'].shape[1]\n",
    "\n",
    "samples = model.sample(train_epoch[i_idx]['x_context'][:, :, None].to(device),\n",
    "                       train_epoch[i_idx]['y_context'][:, 0, :, None].to(device) / 100 + 1e-2,\n",
    "                       x_plot,\n",
    "                       num_samples=100,\n",
    "                       noiseless=False,\n",
    "                       double=True)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x_plot[0, :, 0].cpu().detach().numpy(),\n",
    "         samples[:, b_idx, :].cpu().detach().numpy().T,\n",
    "         color='green',\n",
    "         alpha=0.02,\n",
    "         zorder=1)\n",
    "\n",
    "plt.plot(x_plot[0, :, 0].cpu().detach().numpy(),\n",
    "         torch.mean(samples[:, b_idx, :], axis=0).cpu().detach().numpy().T,\n",
    "         color='black',\n",
    "         alpha=0.5,\n",
    "         zorder=2)\n",
    "\n",
    "plt.scatter(train_epoch[i_idx]['x_context'][b_idx, :].detach().numpy(),\n",
    "            train_epoch[i_idx]['y_context'][b_idx, 0, :].detach().numpy() / 100,\n",
    "            color='black',\n",
    "            zorder=2)\n",
    "\n",
    "# plt.scatter(train_epoch[i_idx]['x_context'][b_idx, max_ctx:].detach().numpy(),\n",
    "#             train_epoch[i_idx]['y_context'][b_idx, 0, max_ctx:].detach().numpy() / 100,\n",
    "#             color='red',\n",
    "#             zorder=2)\n",
    "\n",
    "plt.scatter(train_epoch[i_idx]['x_target'][b_idx, :].detach().numpy(),\n",
    "            train_epoch[i_idx]['y_target'][b_idx, 0, :].detach().numpy() / 100,\n",
    "            color='red',\n",
    "            zorder=2)\n",
    "plt.ylim([0, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f744c42-a03f-4ac6-b1be-581d98b8fc9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-gnp",
   "language": "python",
   "name": "venv-gnp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
