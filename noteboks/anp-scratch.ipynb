{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8148dd36-bfa1-45f2-89a2-cc12bc3347c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from cnp.encoders import StandardANPEncoder, ConvEncoder\n",
    "from cnp.decoders import ConvDecoder\n",
    "from cnp.lnp import StandardANP, LatentNeuralProcess\n",
    "from cnp.cov import AddHomoNoise\n",
    "from cnp.utils import move_channel_idx\n",
    "\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae9d3546-f83b-4aca-8887-c5ff9455d9b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 8, 128])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_context = torch.ones(size=(10, 5, 1))\n",
    "x_target = torch.ones(size=(10, 8, 1))\n",
    "y_context = torch.ones(size=(10, 5, 1))\n",
    "y_target = torch.ones(size=(10, 8, 1))\n",
    "\n",
    "encoder = StandardANPEncoder(input_dim=2, latent_dim=128)\n",
    "encoder(x_context, y_context, x_target).sample().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "993ec1f6-b71a-4327-affe-825804e63bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 1\n",
    "add_noise = AddHomoNoise()\n",
    "\n",
    "anp = StandardANP(input_dim=input_dim, add_noise=add_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2893872-0ec7-436f-a033-d1201c0f4c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 10, 8, 8])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = anp(x_context=x_context, y_context=y_context, x_target=x_target, num_samples=100)\n",
    "result[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6088edf3-0991-462e-8650-98e475d94384",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(lr=1e-2, params=anp.parameters())\n",
    "\n",
    "\n",
    "for i in range(1000):\n",
    "    \n",
    "    optim.zero_grad()\n",
    "    \n",
    "    loss = anp.loss(x_context, y_context, x_target, y_target, num_samples=10)\n",
    "    \n",
    "    loss.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "010b9bf3-5a56-45b2-8d44-2aa4d6ec4b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HalfUNet(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 in_channels,\n",
    "                 out_channels):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        conv = getattr(nn, f'Conv{input_dim}d')\n",
    "        convt = getattr(nn, f'ConvTranspose{input_dim}d')\n",
    "        \n",
    "        self.activation = nn.ReLU()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_halving_layers = 6\n",
    "\n",
    "        self.l1 = conv(in_channels=self.in_channels,\n",
    "                       out_channels=self.in_channels,\n",
    "                       kernel_size=5,\n",
    "                       stride=2,\n",
    "                       padding=2)\n",
    "        \n",
    "        self.l2 = conv(in_channels=self.in_channels,\n",
    "                       out_channels=2*self.in_channels,\n",
    "                       kernel_size=5,\n",
    "                       stride=2,\n",
    "                       padding=2)\n",
    "        \n",
    "        self.l3 = conv(in_channels=2*self.in_channels,\n",
    "                       out_channels=2*self.in_channels,\n",
    "                       kernel_size=5,\n",
    "                       stride=2,\n",
    "                       padding=2)\n",
    "            \n",
    "        self.l4 = convt(in_channels=2*self.in_channels,\n",
    "                        out_channels=2*self.in_channels,\n",
    "                        kernel_size=5,\n",
    "                        stride=2,\n",
    "                        padding=2,\n",
    "                        output_padding=1)\n",
    "        \n",
    "        self.l5 = convt(in_channels=4*self.in_channels,\n",
    "                        out_channels=self.in_channels,\n",
    "                        kernel_size=5,\n",
    "                        stride=2,\n",
    "                        padding=2,\n",
    "                        output_padding=1)\n",
    "        \n",
    "        self.l6 = convt(in_channels=2*self.in_channels,\n",
    "                        out_channels=self.in_channels,\n",
    "                        kernel_size=5,\n",
    "                        stride=2,\n",
    "                        padding=2,\n",
    "                        output_padding=1)\n",
    "\n",
    "\n",
    "        self.last_layer_multiplier = conv(in_channels=2*self.in_channels,\n",
    "                                          out_channels=self.out_channels,\n",
    "                                          kernel_size=1,\n",
    "                                          stride=1,\n",
    "                                          padding=0)\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the convolutional structure.\n",
    "\n",
    "        Args:\n",
    "            x (tensor): Inputs of shape `(batch, n_in, in_channels)`.\n",
    "\n",
    "        Returns:\n",
    "            tensor: Outputs of shape `(batch, n_out, out_channels)`.\n",
    "        \"\"\"\n",
    "        \n",
    "        h1 = self.activation(self.l1(x))\n",
    "        h2 = self.activation(self.l2(h1))\n",
    "        h3 = self.activation(self.l3(h2))\n",
    "        h4 = self.activation(self.l4(h3))\n",
    "\n",
    "        h4 = torch.cat([h4, h2], dim=1)\n",
    "        h5 = self.activation(self.l5(h4))\n",
    "\n",
    "        h5 = torch.cat([h5, h1], dim=1)\n",
    "        h6 = self.activation(self.l6(h5))\n",
    "        h6 = torch.cat([x, h6], dim=1)\n",
    "\n",
    "        return self.last_layer_multiplier(h6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25b23c7c-9c85-4b23-a15b-c90e5297b4f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 2, 128])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "half_unet = HalfUNet(input_dim=1,\n",
    "                     in_channels=8,\n",
    "                     out_channels=2)\n",
    "\n",
    "zeros = torch.zeros(size=(100, 8, 128))\n",
    "\n",
    "half_unet(zeros).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1064b835-1fe7-495d-b610-cf65ee62b9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardConvNP(LatentNeuralProcess):\n",
    "    \n",
    "    def __init__(self, input_dim, add_noise):\n",
    "        \n",
    "        # Dimension of output is 1 for scalar outputs -- do not change\n",
    "        output_dim = 1\n",
    "        \n",
    "        # Num channels of input passed to encoder CNN\n",
    "        encoder_conv_input_channels = 8\n",
    "        \n",
    "        # Num channels of latent function -- outputted by encoder, expected by decoder\n",
    "        latent_function_channels = 1\n",
    "        \n",
    "        # Num channels of output of decoder CNN\n",
    "        decoder_conv_output_channels = 8\n",
    "        \n",
    "        # Num channels of output of decoder\n",
    "        decoder_out_channels = 32\n",
    "        \n",
    "        # Encoder convolutional architecture\n",
    "        encoder_conv = HalfUNet(input_dim=input_dim,\n",
    "                                in_channels=encoder_conv_input_channels,\n",
    "                                out_channels=2*latent_function_channels)\n",
    "        \n",
    "        # Encoder convolutional architecture\n",
    "        decoder_conv = HalfUNet(input_dim=input_dim,\n",
    "                                in_channels=latent_function_channels,\n",
    "                                out_channels=decoder_conv_output_channels)\n",
    "\n",
    "        # Construct the convolutional encoder\n",
    "        grid_multiplier =  2 ** encoder_conv.num_halving_layers\n",
    "        points_per_unit = 32\n",
    "        init_length_scale = 2.0 / points_per_unit\n",
    "        grid_margin = 0.2\n",
    "        \n",
    "        encoder = LatentConvEncoder(input_dim=input_dim,\n",
    "                                    conv_architecture=encoder_conv,\n",
    "                                    init_length_scale=init_length_scale, \n",
    "                                    points_per_unit=points_per_unit, \n",
    "                                    grid_multiplier=grid_multiplier,\n",
    "                                    grid_margin=grid_margin)\n",
    "        \n",
    "        decoder = ConvDecoder(input_dim=input_dim,\n",
    "                              conv_architecture=decoder_conv,\n",
    "                              conv_out_channels=decoder_conv.out_channels,\n",
    "                              out_channels=2*decoder_out_channels,\n",
    "                              init_length_scale=init_length_scale,\n",
    "                              points_per_unit=points_per_unit,\n",
    "                              grid_multiplier=grid_multiplier,\n",
    "                              grid_margin=grid_margin)\n",
    "\n",
    "\n",
    "        super().__init__(encoder=encoder,\n",
    "                         decoder=decoder,\n",
    "                         add_noise=add_noise)\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        \n",
    "class LatentConvEncoder(ConvEncoder):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 conv_architecture,\n",
    "                 init_length_scale, \n",
    "                 points_per_unit, \n",
    "                 grid_multiplier,\n",
    "                 grid_margin):\n",
    "        \n",
    "        self.conv_input_channels = conv_architecture.in_channels\n",
    "        self.conv_output_channels = conv_architecture.out_channels // 2\n",
    "        \n",
    "        super().__init__(input_dim=input_dim, \n",
    "                         out_channels=self.conv_input_channels, \n",
    "                         init_length_scale=init_length_scale, \n",
    "                         points_per_unit=points_per_unit, \n",
    "                         grid_multiplier=grid_multiplier,\n",
    "                         grid_margin=grid_margin)\n",
    "        \n",
    "        self.conv_architecture = conv_architecture\n",
    "        \n",
    "        \n",
    "    def forward(self, x_context, y_context, x_target):\n",
    "        \n",
    "        r = super().forward(x_context, y_context, x_context)\n",
    "        r = self.conv_architecture(r)\n",
    "        \n",
    "        mean = r[:, ::2]\n",
    "        scale = torch.exp(r[:, 1::2])\n",
    "        \n",
    "        distribution = torch.distributions.Normal(loc=mean, scale=scale)\n",
    "        \n",
    "        return distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d8ea2645-4cd6-4ede-a8a6-328927869958",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_context = torch.randn(size=(10, 5, 1))\n",
    "x_target = torch.randn(size=(10, 8, 1))\n",
    "\n",
    "y_context = torch.randn(size=(10, 5, 1))\n",
    "y_target = torch.randn(size=(10, 8, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a9a1319-5feb-43e3-a20f-74bd398b802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "convnp = StandardConvNP(input_dim=1, add_noise=AddHomoNoise())\n",
    "mean, cov = convnp(x_context, y_context, x_target, num_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c6c2f81-091e-4f63-a3d8-58c2e93faa49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6421, grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convnp.loss(x_context, y_context, x_target, y_target, num_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "53956d67-2055-4303-aa4f-aa97d31728be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optim = torch.optim.Adam(lr=1e-2, params=convnp.parameters())\n",
    "\n",
    "# for i in range(1000):\n",
    "    \n",
    "#     optim.zero_grad()\n",
    "    \n",
    "#     loss = convnp.loss(x_context, y_context, x_target, y_target, num_samples=10)\n",
    "    \n",
    "#     print(loss)\n",
    "    \n",
    "#     loss.backward()\n",
    "#     optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597dd573-5152-48c0-8fad-973d2bc66e73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
